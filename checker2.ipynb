{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from laplace import Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from utils.DataUtils import save_datasets, load_datasets, extract_samples_from_unlabeled, delete_samples_from_unlabeled, add_samples_to_labeled, add_and_extract_and_delete_samples\n",
    "from utils.ModelUtils import initialize_model_weights, train_model, evaluate, count_parameters, save_model, load_model\n",
    "from utils.LaplaceUtils import return_hessian_eigenvalues, compute_outcome_hessian_from_model, symmetric_matrix_sqrt, fast_jacobian, low_rank_updated_part\n",
    "from utils.AlFunctions import DoptScore_per_sample, AoptScore_per_sample, ToptScore_per_sample, selection_AL_scores, AL_finetune_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62344fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:15<00:00, 631kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 153kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 926kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MNIST loaded\n",
      "   Train set: <torch.utils.data.dataset.TensorDataset object at 0x0000021A1714AD70>, n_train = 60000\n",
      "   Test set: <torch.utils.data.dataset.TensorDataset object at 0x0000021A17149090>, n_test = 10000\n",
      "   Input dim: 784\n",
      "   Output classes: 10\n"
     ]
    }
   ],
   "source": [
    "'''Dataset Loader for MNIST dataset'''\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# MNIST 다운로드 및 로드\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean, std\n",
    "])\n",
    "\n",
    "# Train/Test split\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Flatten images (28x28 -> 784)\n",
    "Xtr = mnist_train.data.numpy().reshape(-1, 784).astype(np.float32) / 255.0\n",
    "ytr = mnist_train.targets.numpy().astype(np.int64)\n",
    "Xte = mnist_test.data.numpy().reshape(-1, 784).astype(np.float32) / 255.0\n",
    "yte = mnist_test.targets.numpy().astype(np.int64)\n",
    "\n",
    "# Standardization (optional, 이미 normalize 했지만 추가 스케일링 원하면)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr = scaler.transform(Xtr)\n",
    "Xte = scaler.transform(Xte)\n",
    "\n",
    "# Convert to tensors\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "ytr_t = torch.from_numpy(ytr).long()\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte).long()\n",
    "\n",
    "# TensorDataset\n",
    "train_set = TensorDataset(Xtr_t, ytr_t)\n",
    "test_set = TensorDataset(Xte_t, yte_t)\n",
    "\n",
    "print(f\"✅ MNIST loaded\")\n",
    "print(f\"   Train set: {train_set}, n_train = {len(train_set)}\")\n",
    "print(f\"   Test set: {test_set}, n_test = {len(test_set)}\")\n",
    "print(f\"   Input dim: {Xtr_t.shape[1]}\")\n",
    "print(f\"   Output classes: {len(torch.unique(ytr_t))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc61189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data 준비 (기존 Xtr_t, ytr_t 사용)\n",
    "digits = load_digits()\n",
    "x, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(x, y, test_size=0.25, stratify=y, random_state=42)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)          # (N_train, D)\n",
    "ytr_t = torch.from_numpy(ytr).long()   # (N_train,)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "num_pretrain = 30\n",
    "indices = list(range(len(Xtr_t)))\n",
    "random.shuffle(indices)\n",
    "labeled_indices = indices[:num_pretrain]       # 실제 dataset 인덱스들\n",
    "unlabeled_indices = indices[num_pretrain:]\n",
    "\n",
    "# 기본 전체 텐서데이터셋 하나 생성\n",
    "full_train_dataset = TensorDataset(Xtr_t, ytr_t)\n",
    "\n",
    "# Subset으로 라벨/언라벨드 관리 (TensorDataset에 직접 numpy 넣지 않음)\n",
    "train_set_Labeled = Subset(full_train_dataset, labeled_indices)\n",
    "train_set_Unlabeled = Subset(full_train_dataset, unlabeled_indices)\n",
    "test_set = TensorDataset(Xte_t, yte_t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoung_laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
