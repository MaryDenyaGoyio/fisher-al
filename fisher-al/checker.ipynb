{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f79bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Libraries'''\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from laplace import Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "import pickle\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1bb46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001C802668B50>\n"
     ]
    }
   ],
   "source": [
    "'''Dataset Loader for Digits dataset'''\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data\n",
    "digits = load_digits()\n",
    "X, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "ytr_t = torch.from_numpy(ytr)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "train_set = TensorDataset(Xtr_t, ytr_t)\n",
    "print(train_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc5687",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a51551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, only_trainable=False):\n",
    "    \"\"\"\n",
    "    Î™®Îç∏Ïùò ÌååÎùºÎØ∏ÌÑ∞ Í∞úÏàòÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "    only_trainable=TrueÏù¥Î©¥ requires_grad=TrueÏù∏ ÌååÎùºÎØ∏ÌÑ∞Îßå ÏÖâÎãàÎã§.\n",
    "    \"\"\"\n",
    "    params = (p for p in model.parameters() if (not only_trainable) or p.requires_grad)\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "def get_models():\n",
    "    models = []\n",
    "\n",
    "    # 1. Í∞ÄÏû• Îã®ÏàúÌïú Î™®Îç∏\n",
    "    models.append(nn.Linear(64, 10))\n",
    "\n",
    "    # 2. ÌååÎùºÎØ∏ÌÑ∞ 2Î∞∞: Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 3. Îçî ÍπäÍ≤å: Linear + Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 4. Îçî ÍπäÍ≥† ÎÑìÍ≤å\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 5. BatchNorm Ï∂îÍ∞Ä\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ))\n",
    "\n",
    "    # 6. Dropout Ï∂îÍ∞Ä\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 7. Îçî ÍπäÍ≤å, Îçî ÎÑìÍ≤å\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 8. Îçî ÎßéÏùÄ Î†àÏù¥Ïñ¥ÏôÄ BatchNorm, Dropout\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 9. Îçî ÍπäÍ≥† ÎÑìÍ≤å, ÌôúÏÑ±Ìôî Îã§ÏñëÌôî\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 10. Í∞ÄÏû• ÌÅ∞ Î™®Îç∏\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate(loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "def cumulative_explained_ratio(val, alpha=0.9):\n",
    "    \"\"\"\n",
    "    val: list or np.array of eigenvalues (can be positive/negative)\n",
    "    alpha: target cumulative proportion (e.g., 0.9)\n",
    "    \n",
    "    Returns:\n",
    "        ratio (float): index/p where cumulative sum first exceeds alpha\n",
    "        idx (int): the actual index achieving it\n",
    "    \"\"\"\n",
    "    val = np.array(val, dtype=float)\n",
    "    # ÏùåÏàò Í∞íÏùÄ curvature ÏÑ§Î™ÖÏóê Í∏∞Ïó¨ÌïòÏßÄ ÏïäÎèÑÎ°ù Ï†úÍ±∞ (ÌïÑÏöîÏãú ÏòµÏÖòÌôî Í∞ÄÎä•)\n",
    "    val = np.maximum(val, 0)\n",
    "    \n",
    "    if np.sum(val) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    sorted_vals = np.sort(val)[::-1]  # ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨\n",
    "    cumvals = np.cumsum(sorted_vals)\n",
    "    total = cumvals[-1]\n",
    "    threshold = alpha * total\n",
    "\n",
    "    idx = np.searchsorted(cumvals, threshold)\n",
    "    ratio = (idx + 1) / len(val)\n",
    "    return ratio, idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 10\n",
    "alpha = 0.9\n",
    "results = {}\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"\\n=== Repetition {repeat+1}/{n_repeats} ===\")\n",
    "    \n",
    "    # üí° Îß§ Î∞òÎ≥µÎßàÎã§ fresh initialization\n",
    "    models = get_models()\n",
    "\n",
    "    for model in models:\n",
    "        n_params = count_parameters(model)\n",
    "        x_val = np.log(n_params)\n",
    "\n",
    "        print(f\"\\nTraining model with {n_params} parameters (repeat {repeat+1})\")\n",
    "\n",
    "        torch.manual_seed(repeat)\n",
    "        np.random.seed(repeat)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "        train_eval_loader = DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "\n",
    "        n_epochs_monitor = 30\n",
    "        for epoch in range(n_epochs_monitor):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # after training:\n",
    "        la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "        la.fit(train_loader)\n",
    "        val = la.H.cpu().numpy()\n",
    "\n",
    "        ratio, idx = cumulative_explained_ratio(val, alpha=alpha)\n",
    "\n",
    "        # üíæ ratio + full eigenvalues Îëò Îã§ Ï†ÄÏû•\n",
    "        results.setdefault(x_val, []).append({\n",
    "            'ratio': ratio,\n",
    "            'eigenvalues': val\n",
    "        })\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0c6c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved logdet/trace plot ‚Üí logdet_vs_tracebound.png\n",
      "‚úÖ Saved cumulative ratio plot ‚Üí cumulative_explained_ratio.png\n"
     ]
    }
   ],
   "source": [
    "def _safe_extract_eigvals(obj):\n",
    "    \"\"\"\n",
    "    Try to find an eigenvalue array (np.ndarray or list-like) inside obj.\n",
    "    Return a 1D numpy array if found, else return None.\n",
    "    Handles: np.ndarray, list/tuple of arrays, dict with keys like 'eigenvalues','eigvals','H','values',\n",
    "    and nested structures.\n",
    "    \"\"\"\n",
    "    # direct numpy array\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.flatten()\n",
    "\n",
    "    # numpy scalar or Python scalar -> not an eigenvalue array\n",
    "    if isinstance(obj, (np.floating, np.integer, float, int, np.float64, np.int64)):\n",
    "        return None\n",
    "\n",
    "    # list/tuple -> try each element\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            arr = _safe_extract_eigvals(item)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # dict -> try common keys first, then recurse over values\n",
    "    if isinstance(obj, dict):\n",
    "        # common possible keys that hold eigenvalues\n",
    "        preferred_keys = ['eigenvalues', 'eigvals', 'eigvals_', 'H', 'values', 'vals', 'eigs', 'eigen']\n",
    "        for k in preferred_keys:\n",
    "            if k in obj:\n",
    "                arr = _safe_extract_eigvals(obj[k])\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "\n",
    "        # if none of preferred keys, recurse values (but skip scalar values)\n",
    "        for v in obj.values():\n",
    "            arr = _safe_extract_eigvals(v)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # other types (pandas, torch tensor) - handle torch tensors\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.detach().cpu().numpy().flatten()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: not recognized\n",
    "    return None\n",
    "\n",
    "\n",
    "def plot_cumulative_explained_ratio(results, alpha=0.9, save_path=None):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ either floats OR dicts with 'ratio' key ] }\n",
    "    Draws median +/- std errorbar and saves figure.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    medians = []\n",
    "    stds = []\n",
    "    for x_val, items in results.items():\n",
    "        ratios = []\n",
    "        # items may be list of floats or list of dicts or mixed\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and 'ratio' in it:\n",
    "                try:\n",
    "                    ratios.append(float(it['ratio']))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif isinstance(it, (float, int, np.floating, np.integer)):\n",
    "                ratios.append(float(it))\n",
    "            else:\n",
    "                # maybe nested dict with ratio inside\n",
    "                if isinstance(it, dict):\n",
    "                    for v in it.values():\n",
    "                        if isinstance(v, (float, int, np.floating, np.integer)):\n",
    "                            ratios.append(float(v))\n",
    "                            break\n",
    "        if len(ratios) == 0:\n",
    "            # skip if nothing found\n",
    "            print(f\"‚ö†Ô∏è  No ratio entries found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "        xs.append(float(x_val))\n",
    "        medians.append(np.median(ratios))\n",
    "        stds.append(np.std(ratios))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"‚ö†Ô∏è No ratio data found in results. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    medians = np.array(medians)[order]\n",
    "    stds = np.array(stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, medians, yerr=stds, fmt='o-', capsize=5, ecolor='gray', elinewidth=1.5)\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(f\"Cumulative explained ratio (Œ±={alpha})\")\n",
    "    plt.title(\"Model Complexity vs. Curvature Concentration\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/cumulative_explained_ratio_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        # ensure directory exists\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved cumulative ratio plot ‚Üí {save_path}\")\n",
    "\n",
    "def plot_logdet_vs_tracebound(results, save_path=None, clip_eps=1e-8):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ items ] }\n",
    "    Each item can be:\n",
    "      - a dict containing 'eigenvalues' (or 'eigvals', 'H', etc.)\n",
    "      - a direct np.ndarray of eigenvalues\n",
    "      - nested lists/dicts\n",
    "    This computes per-trial:\n",
    "      logdet = sum(log(clipped_eigvals))\n",
    "      trace_bound = d * log(mean(clipped_eigvals))\n",
    "    Then plots median +/- std across trials for each x (log n_params).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    logdet_medians = []\n",
    "    logdet_stds = []\n",
    "    trace_medians = []\n",
    "    trace_stds = []\n",
    "\n",
    "    for x_val, items in results.items():\n",
    "        # collect all eig arrays found under this x_val\n",
    "        eig_lists = []\n",
    "        for it in items:\n",
    "            arr = _safe_extract_eigvals(it)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            # convert to numpy and flatten\n",
    "            try:\n",
    "                arr = np.array(arr, dtype=float).flatten()\n",
    "            except Exception:\n",
    "                continue\n",
    "            eig_lists.append(arr)\n",
    "\n",
    "        if len(eig_lists) == 0:\n",
    "            print(f\"‚ö†Ô∏è No eigenvalue arrays found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        logdets = []\n",
    "        tracebounds = []\n",
    "        for eig in eig_lists:\n",
    "            # sanitize values\n",
    "            eig = eig[np.isfinite(eig)]\n",
    "            if eig.size == 0:\n",
    "                continue\n",
    "            eig = np.clip(eig, clip_eps, None)  # avoid non-positive\n",
    "            d = eig.size\n",
    "            if d == 0:\n",
    "                continue\n",
    "            logdet = np.sum(np.log(eig))\n",
    "            tracebound = d * np.log(np.sum(eig) / d)\n",
    "            logdets.append(logdet)\n",
    "            tracebounds.append(tracebound)\n",
    "\n",
    "        if len(logdets) == 0:\n",
    "            print(f\"‚ö†Ô∏è After sanitization no valid eigvals for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        xs.append(float(x_val))\n",
    "        logdet_medians.append(np.median(logdets))\n",
    "        logdet_stds.append(np.std(logdets))\n",
    "        trace_medians.append(np.median(tracebounds))\n",
    "        trace_stds.append(np.std(tracebounds))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"‚ö†Ô∏è No valid data to plot for logdet vs tracebound.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    logdet_medians = np.array(logdet_medians)[order]\n",
    "    logdet_stds = np.array(logdet_stds)[order]\n",
    "    trace_medians = np.array(trace_medians)[order]\n",
    "    trace_stds = np.array(trace_stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, logdet_medians, yerr=logdet_stds, fmt='o-', label='log|H| (‚àë log Œª_i)')\n",
    "    plt.errorbar(xs, trace_medians, yerr=trace_stds, fmt='s--', label='trace bound (d¬∑log(tr/d))')\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Log-determinant vs. Trace Bound (Curvature Scale)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/logdet_vs_tracebound_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved logdet/trace plot ‚Üí {save_path}\")\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "plot_logdet_vs_tracebound(results, save_path='logdet_vs_tracebound.png')\n",
    "plot_cumulative_explained_ratio(results, alpha=0.9, save_path='cumulative_explained_ratio.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730900c",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684ae6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoung_laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
