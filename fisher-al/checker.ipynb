{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Libraries'''\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from laplace import Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bb46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001607C40BBE0>\n"
     ]
    }
   ],
   "source": [
    "'''Dataset Loader for Digits dataset'''\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data\n",
    "digits = load_digits()\n",
    "X, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "ytr_t = torch.from_numpy(ytr)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "train_set = TensorDataset(Xtr_t, ytr_t)\n",
    "print(train_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc5687",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a51551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, only_trainable=False):\n",
    "    \"\"\"\n",
    "    Î™®Îç∏Ïùò ÌååÎùºÎØ∏ÌÑ∞ Í∞úÏàòÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n",
    "    only_trainable=TrueÏù¥Î©¥ requires_grad=TrueÏù∏ ÌååÎùºÎØ∏ÌÑ∞Îßå ÏÖâÎãàÎã§.\n",
    "    \"\"\"\n",
    "    params = (p for p in model.parameters() if (not only_trainable) or p.requires_grad)\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "def get_models():\n",
    "    models = []\n",
    "\n",
    "    # 1. Í∞ÄÏû• Îã®ÏàúÌïú Î™®Îç∏\n",
    "    models.append(nn.Linear(64, 10))\n",
    "\n",
    "    # 2. ÌååÎùºÎØ∏ÌÑ∞ 2Î∞∞: Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 3. Îçî ÍπäÍ≤å: Linear + Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 4. Îçî ÍπäÍ≥† ÎÑìÍ≤å\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 5. BatchNorm Ï∂îÍ∞Ä\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ))\n",
    "\n",
    "    # 6. Dropout Ï∂îÍ∞Ä\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 7. Îçî ÍπäÍ≤å, Îçî ÎÑìÍ≤å\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 8. Îçî ÎßéÏùÄ Î†àÏù¥Ïñ¥ÏôÄ BatchNorm, Dropout\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 9. Îçî ÍπäÍ≥† ÎÑìÍ≤å, ÌôúÏÑ±Ìôî Îã§ÏñëÌôî\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 10. Í∞ÄÏû• ÌÅ∞ Î™®Îç∏\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate(loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "def cumulative_explained_ratio(val, alpha=0.9):\n",
    "    \"\"\"\n",
    "    val: list or np.array of eigenvalues (can be positive/negative)\n",
    "    alpha: target cumulative proportion (e.g., 0.9)\n",
    "    \n",
    "    Returns:\n",
    "        ratio (float): index/p where cumulative sum first exceeds alpha\n",
    "        idx (int): the actual index achieving it\n",
    "    \"\"\"\n",
    "    val = np.array(val, dtype=float)\n",
    "    # ÏùåÏàò Í∞íÏùÄ curvature ÏÑ§Î™ÖÏóê Í∏∞Ïó¨ÌïòÏßÄ ÏïäÎèÑÎ°ù Ï†úÍ±∞ (ÌïÑÏöîÏãú ÏòµÏÖòÌôî Í∞ÄÎä•)\n",
    "    val = np.maximum(val, 0)\n",
    "    \n",
    "    if np.sum(val) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    sorted_vals = np.sort(val)[::-1]  # ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨\n",
    "    cumvals = np.cumsum(sorted_vals)\n",
    "    total = cumvals[-1]\n",
    "    threshold = alpha * total\n",
    "\n",
    "    idx = np.searchsorted(cumvals, threshold)\n",
    "    ratio = (idx + 1) / len(val)\n",
    "    return ratio, idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 10\n",
    "alpha = 0.9\n",
    "results = {}\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"\\n=== Repetition {repeat+1}/{n_repeats} ===\")\n",
    "    \n",
    "    # üí° Îß§ Î∞òÎ≥µÎßàÎã§ fresh initialization\n",
    "    models = get_models()\n",
    "\n",
    "    for model in models:\n",
    "        n_params = count_parameters(model)\n",
    "        x_val = np.log(n_params)\n",
    "\n",
    "        print(f\"\\nTraining model with {n_params} parameters (repeat {repeat+1})\")\n",
    "\n",
    "        torch.manual_seed(repeat)\n",
    "        np.random.seed(repeat)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "        train_eval_loader = DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "\n",
    "        n_epochs_monitor = 30\n",
    "        for epoch in range(n_epochs_monitor):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # after training:\n",
    "        la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "        la.fit(train_loader)\n",
    "        val = la.H.cpu().numpy()\n",
    "\n",
    "        ratio, idx = cumulative_explained_ratio(val, alpha=alpha)\n",
    "\n",
    "        # üíæ ratio + full eigenvalues Îëò Îã§ Ï†ÄÏû•\n",
    "        results.setdefault(x_val, []).append({\n",
    "            'ratio': ratio,\n",
    "            'eigenvalues': val\n",
    "        })\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved logdet/trace plot ‚Üí logdet_vs_tracebound.png\n",
      "‚úÖ Saved cumulative ratio plot ‚Üí cumulative_explained_ratio.png\n"
     ]
    }
   ],
   "source": [
    "def _safe_extract_eigvals(obj):\n",
    "    \"\"\"\n",
    "    Try to find an eigenvalue array (np.ndarray or list-like) inside obj.\n",
    "    Return a 1D numpy array if found, else return None.\n",
    "    Handles: np.ndarray, list/tuple of arrays, dict with keys like 'eigenvalues','eigvals','H','values',\n",
    "    and nested structures.\n",
    "    \"\"\"\n",
    "    # direct numpy array\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.flatten()\n",
    "\n",
    "    # numpy scalar or Python scalar -> not an eigenvalue array\n",
    "    if isinstance(obj, (np.floating, np.integer, float, int, np.float64, np.int64)):\n",
    "        return None\n",
    "\n",
    "    # list/tuple -> try each element\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            arr = _safe_extract_eigvals(item)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # dict -> try common keys first, then recurse over values\n",
    "    if isinstance(obj, dict):\n",
    "        # common possible keys that hold eigenvalues\n",
    "        preferred_keys = ['eigenvalues', 'eigvals', 'eigvals_', 'H', 'values', 'vals', 'eigs', 'eigen']\n",
    "        for k in preferred_keys:\n",
    "            if k in obj:\n",
    "                arr = _safe_extract_eigvals(obj[k])\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "\n",
    "        # if none of preferred keys, recurse values (but skip scalar values)\n",
    "        for v in obj.values():\n",
    "            arr = _safe_extract_eigvals(v)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # other types (pandas, torch tensor) - handle torch tensors\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.detach().cpu().numpy().flatten()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: not recognized\n",
    "    return None\n",
    "\n",
    "def plot_cumulative_explained_ratio(results, alpha=0.9, save_path=None):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ either floats OR dicts with 'ratio' key ] }\n",
    "    Draws median +/- std errorbar and saves figure.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    medians = []\n",
    "    stds = []\n",
    "    for x_val, items in results.items():\n",
    "        ratios = []\n",
    "        # items may be list of floats or list of dicts or mixed\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and 'ratio' in it:\n",
    "                try:\n",
    "                    ratios.append(float(it['ratio']))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif isinstance(it, (float, int, np.floating, np.integer)):\n",
    "                ratios.append(float(it))\n",
    "            else:\n",
    "                # maybe nested dict with ratio inside\n",
    "                if isinstance(it, dict):\n",
    "                    for v in it.values():\n",
    "                        if isinstance(v, (float, int, np.floating, np.integer)):\n",
    "                            ratios.append(float(v))\n",
    "                            break\n",
    "        if len(ratios) == 0:\n",
    "            # skip if nothing found\n",
    "            print(f\"‚ö†Ô∏è  No ratio entries found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "        xs.append(float(x_val))\n",
    "        medians.append(np.median(ratios))\n",
    "        stds.append(np.std(ratios))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"‚ö†Ô∏è No ratio data found in results. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    medians = np.array(medians)[order]\n",
    "    stds = np.array(stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, medians, yerr=stds, fmt='o-', capsize=5, ecolor='gray', elinewidth=1.5)\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(f\"Cumulative explained ratio (Œ±={alpha})\")\n",
    "    plt.title(\"Model Complexity vs. Curvature Concentration\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/cumulative_explained_ratio_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        # ensure directory exists\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved cumulative ratio plot ‚Üí {save_path}\")\n",
    "\n",
    "def plot_logdet_vs_tracebound(results, save_path=None, clip_eps=1e-8):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ items ] }\n",
    "    Each item can be:\n",
    "      - a dict containing 'eigenvalues' (or 'eigvals', 'H', etc.)\n",
    "      - a direct np.ndarray of eigenvalues\n",
    "      - nested lists/dicts\n",
    "    This computes per-trial:\n",
    "      logdet = sum(log(clipped_eigvals))\n",
    "      trace_bound = d * log(mean(clipped_eigvals))\n",
    "    Then plots median +/- std across trials for each x (log n_params).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    logdet_medians = []\n",
    "    logdet_stds = []\n",
    "    trace_medians = []\n",
    "    trace_stds = []\n",
    "\n",
    "    for x_val, items in results.items():\n",
    "        # collect all eig arrays found under this x_val\n",
    "        eig_lists = []\n",
    "        for it in items:\n",
    "            arr = _safe_extract_eigvals(it)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            # convert to numpy and flatten\n",
    "            try:\n",
    "                arr = np.array(arr, dtype=float).flatten()\n",
    "            except Exception:\n",
    "                continue\n",
    "            eig_lists.append(arr)\n",
    "\n",
    "        if len(eig_lists) == 0:\n",
    "            print(f\"‚ö†Ô∏è No eigenvalue arrays found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        logdets = []\n",
    "        tracebounds = []\n",
    "        for eig in eig_lists:\n",
    "            # sanitize values\n",
    "            eig = eig[np.isfinite(eig)]\n",
    "            if eig.size == 0:\n",
    "                continue\n",
    "            eig = np.clip(eig, clip_eps, None)  # avoid non-positive\n",
    "            d = eig.size\n",
    "            if d == 0:\n",
    "                continue\n",
    "            logdet = np.sum(np.log(eig))\n",
    "            tracebound = d * np.log(np.sum(eig) / d)\n",
    "            logdets.append(logdet)\n",
    "            tracebounds.append(tracebound)\n",
    "\n",
    "        if len(logdets) == 0:\n",
    "            print(f\"‚ö†Ô∏è After sanitization no valid eigvals for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        xs.append(float(x_val))\n",
    "        logdet_medians.append(np.median(logdets))\n",
    "        logdet_stds.append(np.std(logdets))\n",
    "        trace_medians.append(np.median(tracebounds))\n",
    "        trace_stds.append(np.std(tracebounds))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"‚ö†Ô∏è No valid data to plot for logdet vs tracebound.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    logdet_medians = np.array(logdet_medians)[order]\n",
    "    logdet_stds = np.array(logdet_stds)[order]\n",
    "    trace_medians = np.array(trace_medians)[order]\n",
    "    trace_stds = np.array(trace_stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, logdet_medians, yerr=logdet_stds, fmt='o-', label='log|H| (‚àë log Œª_i)')\n",
    "    plt.errorbar(xs, trace_medians, yerr=trace_stds, fmt='s--', label='trace bound (d¬∑log(tr/d))')\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Log-determinant vs. Trace Bound (Curvature Scale)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/logdet_vs_tracebound_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved logdet/trace plot ‚Üí {save_path}\")\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "plot_logdet_vs_tracebound(results, save_path='logdet_vs_tracebound.png')\n",
    "plot_cumulative_explained_ratio(results, alpha=0.9, save_path='cumulative_explained_ratio.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730900c",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabeled_train_loader = DataLoader(labeled_train_set, batch_size=32, shuffle=True)\\nunlabeled_train_loader = DataLoader(unlabeled_train_set, batch_size=32, shuffle=True)\\ntest_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data Ï§ÄÎπÑ (Í∏∞Ï°¥ Xtr_t, ytr_t ÏÇ¨Ïö©)\n",
    "digits = load_digits()\n",
    "x, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(x, y, test_size=0.25, stratify=y, random_state=42)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)          # (N_train, D)\n",
    "ytr_t = torch.from_numpy(ytr).long()   # (N_train,)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "num_pretrain = 30\n",
    "indices = list(range(len(Xtr_t)))\n",
    "random.shuffle(indices)\n",
    "labeled_indices = indices[:num_pretrain]       # Ïã§Ï†ú dataset Ïù∏Îç±Ïä§Îì§\n",
    "unlabeled_indices = indices[num_pretrain:]\n",
    "\n",
    "# Í∏∞Î≥∏ Ï†ÑÏ≤¥ ÌÖêÏÑúÎç∞Ïù¥ÌÑ∞ÏÖã ÌïòÎÇò ÏÉùÏÑ±\n",
    "full_train_dataset = TensorDataset(Xtr_t, ytr_t)\n",
    "\n",
    "# SubsetÏúºÎ°ú ÎùºÎ≤®/Ïñ∏ÎùºÎ≤®Îìú Í¥ÄÎ¶¨ (TensorDatasetÏóê ÏßÅÏ†ë numpy ÎÑ£ÏßÄ ÏïäÏùå)\n",
    "train_set_Labeled = Subset(full_train_dataset, labeled_indices)\n",
    "train_set_Unlabeled = Subset(full_train_dataset, unlabeled_indices)\n",
    "test_set = TensorDataset(Xte_t, yte_t)\n",
    "\n",
    "\n",
    "'''\n",
    "labeled_train_loader = DataLoader(labeled_train_set, batch_size=32, shuffle=True)\n",
    "unlabeled_train_loader = DataLoader(unlabeled_train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 1.4624e+00, 3.4087e+00, 4.6034e+00, 3.4122e+00, 3.8105e+00,\n",
      "        8.0984e+00, 4.9618e-01, 1.2243e-02, 3.2575e+00, 2.9832e+00, 2.3532e+00,\n",
      "        2.4032e+00, 3.5176e+00, 1.0824e+01, 3.1944e+00, 4.5796e-03, 2.7673e+00,\n",
      "        3.3481e+00, 3.6376e+00, 2.8563e+00, 3.2538e+00, 8.6323e+00, 1.2087e+01,\n",
      "        2.2881e-03, 1.5376e+00, 3.1003e+00, 3.2242e+00, 3.1270e+00, 3.7002e+00,\n",
      "        5.4377e+00, 8.6454e+01, 0.0000e+00, 2.2927e+00, 3.1022e+00, 2.6843e+00,\n",
      "        2.5696e+00, 3.1175e+00, 3.5350e+00, 0.0000e+00, 7.9825e+01, 5.3062e+00,\n",
      "        3.6115e+00, 2.3732e+00, 2.2527e+00, 3.7384e+00, 3.4849e+00, 2.2184e-02,\n",
      "        2.2463e+02, 1.4312e+01, 3.4326e+00, 3.7420e+00, 3.0437e+00, 2.0159e+00,\n",
      "        3.0648e+00, 3.6572e+00, 2.2881e-03, 8.6221e-01, 3.2258e+00, 4.5959e+00,\n",
      "        2.3432e+00, 2.5126e+00, 4.7221e+00, 1.6042e+01, 0.0000e+00, 1.3897e+00,\n",
      "        2.5226e+00, 1.8919e+00, 2.8552e+00, 2.2612e+00, 2.6523e+00, 2.7762e-01,\n",
      "        1.0749e-02, 1.7517e+00, 2.1462e+00, 1.8639e+00, 2.6366e+00, 3.3407e+00,\n",
      "        4.1037e+00, 5.0959e-01, 4.0210e-03, 1.5417e+00, 2.1938e+00, 2.9261e+00,\n",
      "        2.6974e+00, 2.9357e+00, 3.1015e+00, 1.9759e+00, 2.0090e-03, 1.4796e+00,\n",
      "        2.5382e+00, 2.3357e+00, 3.2644e+00, 2.8187e+00, 2.6333e+00, 2.7047e+01,\n",
      "        0.0000e+00, 2.9928e+00, 3.0799e+00, 2.3442e+00, 2.6637e+00, 2.7300e+00,\n",
      "        3.5732e+00, 0.0000e+00, 2.4978e+01, 2.4248e+00, 3.7486e+00, 2.0399e+00,\n",
      "        2.2045e+00, 2.6926e+00, 4.5145e+00, 1.9479e-02, 7.0267e+01, 4.8854e+00,\n",
      "        2.4993e+00, 1.9457e+00, 2.9472e+00, 1.7420e+00, 3.6078e+00, 1.2272e+00,\n",
      "        2.0090e-03, 6.5479e-01, 2.2991e+00, 1.7543e+00, 2.3302e+00, 2.8542e+00,\n",
      "        3.1824e+00, 7.1561e+00, 0.0000e+00, 2.0509e+00, 3.3072e+00, 3.8376e+00,\n",
      "        3.4956e+00, 3.4731e+00, 7.2094e+00, 6.2809e-01, 1.2172e-02, 2.8906e+00,\n",
      "        3.0559e+00, 2.2331e+00, 2.3922e+00, 3.6407e+00, 8.9630e+00, 1.7024e+00,\n",
      "        4.5531e-03, 2.4717e+00, 3.3965e+00, 3.8095e+00, 3.0801e+00, 3.2117e+00,\n",
      "        6.1162e+00, 6.9267e+00, 2.2748e-03, 1.5763e+00, 2.7104e+00, 2.8328e+00,\n",
      "        3.1996e+00, 3.4405e+00, 6.0547e+00, 9.8618e+01, 0.0000e+00, 2.4814e+00,\n",
      "        3.2306e+00, 2.6689e+00, 2.7688e+00, 3.2290e+00, 3.6764e+00, 0.0000e+00,\n",
      "        9.1055e+01, 5.9674e+00, 3.6250e+00, 2.2092e+00, 2.3482e+00, 4.0702e+00,\n",
      "        3.0632e+00, 2.2056e-02, 2.5623e+02, 1.6353e+01, 3.6391e+00, 3.3172e+00,\n",
      "        3.0115e+00, 2.4906e+00, 2.7344e+00, 1.6189e+00, 2.2748e-03, 1.0128e+00,\n",
      "        3.2979e+00, 3.8004e+00, 3.2434e+00, 2.7091e+00, 3.6549e+00, 8.4442e+00,\n",
      "        0.0000e+00, 1.4273e+00, 2.4953e+00, 1.7213e+00, 2.6164e+00, 2.2313e+00,\n",
      "        3.1216e+00, 3.1508e-01, 9.2225e-03, 2.2476e+00, 1.6238e+00, 1.8828e+00,\n",
      "        1.9716e+00, 2.7240e+00, 4.4229e+00, 4.4686e-01, 3.4499e-03, 1.9425e+00,\n",
      "        2.1315e+00, 2.4620e+00, 2.4287e+00, 2.2883e+00, 2.7406e+00, 1.4907e+00,\n",
      "        1.7237e-03, 1.2570e+00, 2.3472e+00, 2.2237e+00, 2.3538e+00, 2.6595e+00,\n",
      "        1.7267e+00, 7.2170e-02, 0.0000e+00, 2.1301e+00, 2.5193e+00, 2.3322e+00,\n",
      "        2.0860e+00, 2.2667e+00, 2.7872e+00, 0.0000e+00, 7.2630e-02, 8.7317e-01,\n",
      "        2.9760e+00, 1.8087e+00, 1.8914e+00, 2.6210e+00, 3.6618e+00, 1.6712e-02,\n",
      "        1.7742e-01, 5.0136e-01, 2.2945e+00, 1.9804e+00, 2.4304e+00, 1.6814e+00,\n",
      "        3.0451e+00, 1.4376e+00, 1.7237e-03, 7.9166e-01, 2.2759e+00, 1.8437e+00,\n",
      "        1.8474e+00, 2.2357e+00, 3.0065e+00, 6.9178e+00, 0.0000e+00, 8.4824e-01,\n",
      "        2.0701e+00, 2.0265e+00, 2.2753e+00, 2.2787e+00, 3.3962e+00, 2.4052e-01,\n",
      "        9.1442e-03, 1.7064e+00, 1.5957e+00, 1.7537e+00, 2.0222e+00, 2.6942e+00,\n",
      "        4.4225e+00, 9.8603e-01, 3.4206e-03, 1.4534e+00, 1.9671e+00, 2.4101e+00,\n",
      "        2.2759e+00, 2.4502e+00, 2.7839e+00, 3.4013e+00, 1.7090e-03, 1.0869e+00,\n",
      "        2.3386e+00, 1.9990e+00, 2.5098e+00, 2.5431e+00, 2.0686e+00, 1.6343e-02,\n",
      "        0.0000e+00, 1.9687e+00, 2.6186e+00, 2.0310e+00, 2.0948e+00, 2.1006e+00,\n",
      "        2.6892e+00, 0.0000e+00, 2.1038e-02, 8.5721e-01, 3.1596e+00, 1.8599e+00,\n",
      "        1.9318e+00, 2.0380e+00, 4.4971e+00, 1.6570e-02, 3.2451e-02, 5.4752e-01,\n",
      "        2.2291e+00, 2.0701e+00, 2.4165e+00, 1.3692e+00, 3.5284e+00, 1.0988e+00,\n",
      "        1.7090e-03, 4.7236e-01, 1.8242e+00, 2.0364e+00, 1.8462e+00, 2.3910e+00,\n",
      "        3.0847e+00, 5.7080e+00, 0.0000e+00, 1.8459e+00, 2.9559e+00, 1.9896e+00,\n",
      "        2.3441e+00, 2.1324e+00, 2.3962e+00, 1.8733e-01, 9.6404e-03, 2.7002e+00,\n",
      "        1.9734e+00, 1.8580e+00, 2.0868e+00, 2.6794e+00, 4.2990e+00, 4.3489e-01,\n",
      "        3.6062e-03, 2.3949e+00, 2.5625e+00, 2.5917e+00, 2.3088e+00, 2.6394e+00,\n",
      "        2.6567e+00, 1.4457e+00, 1.8018e-03, 1.5533e+00, 2.5369e+00, 2.4441e+00,\n",
      "        2.6501e+00, 2.8102e+00, 2.0093e+00, 2.3071e-01, 0.0000e+00, 2.6606e+00,\n",
      "        2.8097e+00, 2.4869e+00, 1.9630e+00, 2.3814e+00, 3.3612e+00, 0.0000e+00,\n",
      "        2.1928e-01, 8.1024e-01, 3.0402e+00, 2.1288e+00, 2.1776e+00, 2.7363e+00,\n",
      "        3.5240e+00, 1.7469e-02, 5.8892e-01, 5.3943e-01, 2.2880e+00, 2.1378e+00,\n",
      "        2.5726e+00, 1.8920e+00, 2.9329e+00, 4.9893e+00, 1.8018e-03, 9.3709e-01,\n",
      "        2.6574e+00, 2.1187e+00, 2.3227e+00, 2.1894e+00, 3.2489e+00, 1.7167e+01,\n",
      "        0.0000e+00, 1.8224e+00, 3.0875e+00, 1.9010e+00, 2.7597e+00, 2.6239e+00,\n",
      "        2.8365e+00, 1.9828e-01, 1.0367e-02, 2.6015e+00, 1.7243e+00, 2.1674e+00,\n",
      "        2.0857e+00, 2.9507e+00, 5.8390e+00, 1.1721e+00, 3.8779e-03, 2.3622e+00,\n",
      "        2.5404e+00, 2.5864e+00, 2.7452e+00, 2.4820e+00, 4.4757e+00, 4.0502e+00,\n",
      "        1.9375e-03, 1.2746e+00, 2.6811e+00, 2.6796e+00, 2.4714e+00, 2.7846e+00,\n",
      "        2.2228e+00, 2.0752e-01, 0.0000e+00, 1.8403e+00, 2.8182e+00, 2.8146e+00,\n",
      "        2.4673e+00, 2.3919e+00, 2.8448e+00, 0.0000e+00, 1.9834e-01, 8.9837e-01,\n",
      "        3.2485e+00, 2.1590e+00, 2.1446e+00, 2.9752e+00, 3.7753e+00, 1.8785e-02,\n",
      "        5.2787e-01, 6.0102e-01, 2.5006e+00, 2.3276e+00, 2.7139e+00, 2.0177e+00,\n",
      "        3.1318e+00, 8.2201e-01, 1.9375e-03, 1.1648e+00, 2.7949e+00, 2.1058e+00,\n",
      "        2.1667e+00, 2.3565e+00, 2.0909e+00, 3.6867e+00, 0.0000e+00, 1.9645e+00,\n",
      "        2.4971e+00, 1.3422e+00, 2.1710e+00, 2.2365e+00, 3.5378e+00, 4.9932e-01,\n",
      "        8.9044e-03, 2.1673e+00, 1.3320e+00, 1.7465e+00, 2.1375e+00, 2.7586e+00,\n",
      "        5.0576e+00, 2.8172e-01, 3.3309e-03, 1.9277e+00, 2.6830e+00, 2.1706e+00,\n",
      "        2.3003e+00, 2.5559e+00, 2.8165e+00, 9.0937e-01, 1.6642e-03, 1.1450e+00,\n",
      "        2.2455e+00, 2.3976e+00, 2.3049e+00, 2.8799e+00, 1.9258e+00, 1.3167e-01,\n",
      "        0.0000e+00, 1.7771e+00, 2.2502e+00, 2.2508e+00, 1.6644e+00, 2.3376e+00,\n",
      "        1.8248e+00, 0.0000e+00, 1.2736e-01, 9.1018e-01, 2.8346e+00, 1.6308e+00,\n",
      "        1.4776e+00, 2.7092e+00, 2.7862e+00, 1.6135e-02, 3.3238e-01, 4.6776e-01,\n",
      "        1.8684e+00, 1.7923e+00, 2.2525e+00, 1.8725e+00, 2.4815e+00, 3.8317e+00,\n",
      "        1.6642e-03, 9.8366e-01, 2.2939e+00, 1.4249e+00, 2.4175e+00, 2.2694e+00,\n",
      "        3.7210e+00, 1.4648e+01, 0.0000e+00, 2.5560e+00, 2.9119e+00, 1.4479e+00,\n",
      "        2.2030e+00, 2.3661e+00, 3.4256e+00, 4.9707e-01, 9.9156e-03, 2.8967e+00,\n",
      "        1.7345e+00, 2.0020e+00, 2.0316e+00, 2.8993e+00, 5.2493e+00, 3.0985e-01,\n",
      "        3.7091e-03, 2.6072e+00, 2.6780e+00, 2.4877e+00, 2.4324e+00, 2.7530e+00,\n",
      "        2.7120e+00, 9.9753e-01, 1.8532e-03, 1.2945e+00, 2.2928e+00, 2.3168e+00,\n",
      "        2.5325e+00, 3.0911e+00, 2.7518e+00, 9.6476e-03, 0.0000e+00, 2.0870e+00,\n",
      "        2.5261e+00, 2.3904e+00, 2.1168e+00, 2.5718e+00, 2.6257e+00, 0.0000e+00,\n",
      "        1.5358e-02, 8.6109e-01, 3.0532e+00, 1.9017e+00, 1.9009e+00, 3.2421e+00,\n",
      "        3.2130e+00, 1.7968e-02, 1.4209e-02, 4.7677e-01, 2.1862e+00, 1.9826e+00,\n",
      "        2.1404e+00, 2.4292e+00, 2.6503e+00, 2.4861e+00, 1.8532e-03, 1.1102e+00,\n",
      "        2.8408e+00, 1.6333e+00, 3.3490e+00, 2.4692e+00, 2.6935e+00, 9.3900e+00,\n",
      "        0.0000e+00, 1.4485e+00, 2.7285e+00, 1.9770e+00, 2.2904e+00, 2.1799e+00,\n",
      "        2.5134e+00, 1.5476e-01, 1.0014e-02, 2.3563e+00, 1.7485e+00, 2.1648e+00,\n",
      "        1.9824e+00, 3.0436e+00, 3.5123e+00, 2.4372e-01, 3.7460e-03, 2.1062e+00,\n",
      "        2.3667e+00, 2.5094e+00, 2.5796e+00, 2.5812e+00, 1.6464e+00, 7.6221e-01,\n",
      "        1.8716e-03, 1.1988e+00, 2.4648e+00, 2.4738e+00, 2.5765e+00, 2.5671e+00,\n",
      "        1.8669e+00, 7.5974e-03, 0.0000e+00, 1.9316e+00, 2.7437e+00, 2.5408e+00,\n",
      "        2.3232e+00, 2.1766e+00, 2.7034e+00, 0.0000e+00, 1.3530e-02, 9.0622e-01,\n",
      "        3.3932e+00, 2.0994e+00, 2.0470e+00, 2.3870e+00, 3.9528e+00, 1.8146e-02,\n",
      "        8.7745e-03, 5.8034e-01, 2.5153e+00, 2.2892e+00, 2.8544e+00, 1.8035e+00,\n",
      "        3.2251e+00, 2.2778e+00, 1.8716e-03, 9.2964e-01, 2.3499e+00, 2.0757e+00,\n",
      "        1.9556e+00, 2.3323e+00, 2.5679e+00, 7.9092e+00, 3.0798e+00, 2.7042e+00,\n",
      "        3.0619e+00, 2.3200e+00, 2.3003e+00, 2.4252e+00, 2.6079e+00, 2.2400e+00,\n",
      "        2.4944e+00, 2.5192e+00])\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "model = models[0]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "def train_model(model, train_set_Labeled, criterion, optimizer, n_epochs=10):\n",
    "    n_epochs_monitor = n_epochs\n",
    "    for epoch in range(n_epochs_monitor):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_set_Labeled:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    return model\n",
    "\n",
    "def return_hessian_eigenvalues(model, train_set_Labeled):\n",
    "    la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "    train_loader = DataLoader(train_set_Labeled, batch_size=32, shuffle=True)\n",
    "    la.fit(train_loader)\n",
    "    val = la.H\n",
    "    return val\n",
    "\n",
    "val = return_hessian_eigenvalues(model, train_set_Labeled)\n",
    "\n",
    "print(val)\n",
    "\n",
    "def AL_finetune(model, AL_train_set_Labeled, criterion, optimizer, n_epochs=10):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x160842f5f30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a25353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1091, 2)\n"
     ]
    }
   ],
   "source": [
    "def fast_jacobian(model, x):\n",
    "    model.zero_grad()\n",
    "    # ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)  # (1, input_dim)\n",
    "    b = x.shape[0]\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # precompute param sizes to flatten consistently\n",
    "    param_numels = [p.numel() for p in params]\n",
    "\n",
    "    J_batch = []\n",
    "    for i in range(b):\n",
    "        xi = x[i : i + 1]  # keep batch dim for forward\n",
    "        yi = model(xi)  # (1, d_out)\n",
    "        d_out = yi.shape[-1]\n",
    "\n",
    "        # compute grads for this sample: result (d_out, n_params)\n",
    "        J_i_rows = []\n",
    "        for k in range(d_out):\n",
    "            grads = torch.autograd.grad(yi[0, k], params, retain_graph=True)\n",
    "            grad_flat = torch.cat([g.reshape(-1) for g in grads])\n",
    "            J_i_rows.append(grad_flat)\n",
    "        J_i = torch.stack(J_i_rows, dim=0)  # (d_out, n_params)\n",
    "        J_batch.append(J_i)\n",
    "\n",
    "    J_batch = torch.stack(J_batch, dim=0)  # (b, d_out, n_params)\n",
    "    return J_batch\n",
    "\n",
    "def compute_outcome_hessian_from_model(model, inputs):\n",
    "    # inputs: (d,) or (b, d)\n",
    "    if inputs.ndim == 1:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    z = model(inputs)  # (b, d_out)\n",
    "    p = torch.softmax(z, dim=-1)  # (b, d_out)\n",
    "    # diag_embed builds (b, d_out, d_out)\n",
    "    H = torch.diag_embed(p) - p.unsqueeze(2) * p.unsqueeze(1)  # (b, d_out, d_out)\n",
    "    return H\n",
    "\n",
    "def symmetric_matrix_sqrt(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A: (n,n) or (batch, n, n)\n",
    "    returns: A_sqrt with same shape\n",
    "    \"\"\"\n",
    "    single = (A.dim() == 2)\n",
    "    if single:\n",
    "        A = A.unsqueeze(0)\n",
    "    w, v = torch.linalg.eigh(A)\n",
    "    w_clamped = torch.clamp(w, min=eps)\n",
    "    w_sqrt = torch.sqrt(w_clamped)\n",
    "    A_sqrt = (v * w_sqrt.unsqueeze(-2)) @ v.transpose(-2, -1)\n",
    "    if single:\n",
    "        return A_sqrt[0]\n",
    "    return A_sqrt\n",
    "\n",
    "def low_rank_updated_part(model, x, return_batch: bool = False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - if return_batch=True: U_batch of shape (b, n_params, d_out)\n",
    "      - else: U_all of shape (n_params, b * d_out)  (backward-compatible)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    H = compute_outcome_hessian_from_model(model, x)    # (b, d_out, d_out)\n",
    "    J_batch = fast_jacobian(model, x)                   # (b, d_out, n_params)\n",
    "    H_sqrt = symmetric_matrix_sqrt(H)                   # (b, d_out, d_out)\n",
    "\n",
    "    # J_batch: (b, d_out, n_params) -> transpose -> (b, n_params, d_out)\n",
    "    Jt = J_batch.transpose(1, 2)\n",
    "    U_batch = torch.matmul(Jt, H_sqrt)                  # (b, n_params, d_out)\n",
    "\n",
    "    if return_batch:\n",
    "        return U_batch\n",
    "    b, n_params, d_out = U_batch.shape\n",
    "    U_all = U_batch.permute(1, 0, 2).reshape(n_params, b * d_out)\n",
    "    return U_all\n",
    "\n",
    "def DoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute D-opt score per input sample.\n",
    "    - x: (input_dim,) or (b, input_dim)\n",
    "    - Hessian: torch tensor (n_params,) (diagonal)\n",
    "    Returns: torch tensor shape (b,) with per-sample log-determinant scores\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        # use slogdet for stability\n",
    "        sign, ld = torch.linalg.slogdet(A)\n",
    "        # if numeric issue (sign <=0) return nan for that sample\n",
    "        scores.append(ld if sign > 0 else torch.tensor(float('nan'), device=A.device))\n",
    "    return torch.stack(scores)  # (b,)\n",
    "\n",
    "def AoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute A-opt reduction per input sample.\n",
    "    Returns: torch tensor shape (b,) with per-sample Delta values.\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    deltas = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        A = A + eps * torch.eye(A.shape[0], device=A.device)\n",
    "        S = C.T @ C                          # (d_out, d_out)\n",
    "        X = torch.linalg.solve(A, S)         # (d_out, d_out)\n",
    "        deltas.append(torch.trace(X))\n",
    "    return torch.stack(deltas)  # (b,)\n",
    "\n",
    "\n",
    "models = get_models()\n",
    "model = models[0]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "indices = list(range(len(train_set)))\n",
    "random.shuffle(indices)\n",
    "subset_indices = indices[:256]\n",
    "train_subset = Subset(train_set, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "n_epochs_monitor = 30\n",
    "for epoch in range(n_epochs_monitor):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "la.fit(train_loader)\n",
    "Hessian = la.H\n",
    "Values = []\n",
    "x = train_set[0]\n",
    "'''\n",
    "for i in range(len(train_set) - 256) :\n",
    "    x = train_set[256 + i][0]\n",
    "    Values.append([DoptScore(model, x, Hessian).item(), AoptScore(model, x, Hessian).item()])\n",
    "Values = np.array(Values)\n",
    "print(Values.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c18d4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22.7606], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_set[0][0]\n",
    "AoptScore_per_sample(model, x, Hessian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoung_laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
