{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Libraries'''\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from laplace import Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bb46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001D2D9EABDC0>\n"
     ]
    }
   ],
   "source": [
    "'''Dataset Loader for Digits dataset'''\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data\n",
    "digits = load_digits()\n",
    "X, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "ytr_t = torch.from_numpy(ytr)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "train_set = TensorDataset(Xtr_t, ytr_t)\n",
    "print(train_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc5687",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a51551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, only_trainable=False):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    only_trainable=Trueì´ë©´ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ ì…‰ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    params = (p for p in model.parameters() if (not only_trainable) or p.requires_grad)\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "def get_models():\n",
    "    models = []\n",
    "\n",
    "    # 1. ê°€ìž¥ ë‹¨ìˆœí•œ ëª¨ë¸\n",
    "    models.append(nn.Linear(64, 10))\n",
    "\n",
    "    # 2. íŒŒë¼ë¯¸í„° 2ë°°: Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 3. ë” ê¹Šê²Œ: Linear + Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 4. ë” ê¹Šê³  ë„“ê²Œ\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 5. BatchNorm ì¶”ê°€\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ))\n",
    "\n",
    "    # 6. Dropout ì¶”ê°€\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 7. ë” ê¹Šê²Œ, ë” ë„“ê²Œ\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 8. ë” ë§Žì€ ë ˆì´ì–´ì™€ BatchNorm, Dropout\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 9. ë” ê¹Šê³  ë„“ê²Œ, í™œì„±í™” ë‹¤ì–‘í™”\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 10. ê°€ìž¥ í° ëª¨ë¸\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate(loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "def cumulative_explained_ratio(val, alpha=0.9):\n",
    "    \"\"\"\n",
    "    val: list or np.array of eigenvalues (can be positive/negative)\n",
    "    alpha: target cumulative proportion (e.g., 0.9)\n",
    "    \n",
    "    Returns:\n",
    "        ratio (float): index/p where cumulative sum first exceeds alpha\n",
    "        idx (int): the actual index achieving it\n",
    "    \"\"\"\n",
    "    val = np.array(val, dtype=float)\n",
    "    # ìŒìˆ˜ ê°’ì€ curvature ì„¤ëª…ì— ê¸°ì—¬í•˜ì§€ ì•Šë„ë¡ ì œê±° (í•„ìš”ì‹œ ì˜µì…˜í™” ê°€ëŠ¥)\n",
    "    val = np.maximum(val, 0)\n",
    "    \n",
    "    if np.sum(val) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    sorted_vals = np.sort(val)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    cumvals = np.cumsum(sorted_vals)\n",
    "    total = cumvals[-1]\n",
    "    threshold = alpha * total\n",
    "\n",
    "    idx = np.searchsorted(cumvals, threshold)\n",
    "    ratio = (idx + 1) / len(val)\n",
    "    return ratio, idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 10\n",
    "alpha = 0.9\n",
    "results = {}\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"\\n=== Repetition {repeat+1}/{n_repeats} ===\")\n",
    "    \n",
    "    # ðŸ’¡ ë§¤ ë°˜ë³µë§ˆë‹¤ fresh initialization\n",
    "    models = get_models()\n",
    "\n",
    "    for model in models:\n",
    "        n_params = count_parameters(model)\n",
    "        x_val = np.log(n_params)\n",
    "\n",
    "        print(f\"\\nTraining model with {n_params} parameters (repeat {repeat+1})\")\n",
    "\n",
    "        torch.manual_seed(repeat)\n",
    "        np.random.seed(repeat)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "        train_eval_loader = DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "\n",
    "        n_epochs_monitor = 30\n",
    "        for epoch in range(n_epochs_monitor):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # after training:\n",
    "        la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "        la.fit(train_loader)\n",
    "        val = la.H.cpu().numpy()\n",
    "\n",
    "        ratio, idx = cumulative_explained_ratio(val, alpha=alpha)\n",
    "\n",
    "        # ðŸ’¾ ratio + full eigenvalues ë‘˜ ë‹¤ ì €ìž¥\n",
    "        results.setdefault(x_val, []).append({\n",
    "            'ratio': ratio,\n",
    "            'eigenvalues': val\n",
    "        })\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0c6c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved logdet/trace plot â†’ logdet_vs_tracebound.png\n",
      "âœ… Saved cumulative ratio plot â†’ cumulative_explained_ratio.png\n"
     ]
    }
   ],
   "source": [
    "def _safe_extract_eigvals(obj):\n",
    "    \"\"\"\n",
    "    Try to find an eigenvalue array (np.ndarray or list-like) inside obj.\n",
    "    Return a 1D numpy array if found, else return None.\n",
    "    Handles: np.ndarray, list/tuple of arrays, dict with keys like 'eigenvalues','eigvals','H','values',\n",
    "    and nested structures.\n",
    "    \"\"\"\n",
    "    # direct numpy array\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.flatten()\n",
    "\n",
    "    # numpy scalar or Python scalar -> not an eigenvalue array\n",
    "    if isinstance(obj, (np.floating, np.integer, float, int, np.float64, np.int64)):\n",
    "        return None\n",
    "\n",
    "    # list/tuple -> try each element\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            arr = _safe_extract_eigvals(item)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # dict -> try common keys first, then recurse over values\n",
    "    if isinstance(obj, dict):\n",
    "        # common possible keys that hold eigenvalues\n",
    "        preferred_keys = ['eigenvalues', 'eigvals', 'eigvals_', 'H', 'values', 'vals', 'eigs', 'eigen']\n",
    "        for k in preferred_keys:\n",
    "            if k in obj:\n",
    "                arr = _safe_extract_eigvals(obj[k])\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "\n",
    "        # if none of preferred keys, recurse values (but skip scalar values)\n",
    "        for v in obj.values():\n",
    "            arr = _safe_extract_eigvals(v)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # other types (pandas, torch tensor) - handle torch tensors\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.detach().cpu().numpy().flatten()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: not recognized\n",
    "    return None\n",
    "\n",
    "\n",
    "def plot_cumulative_explained_ratio(results, alpha=0.9, save_path=None):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ either floats OR dicts with 'ratio' key ] }\n",
    "    Draws median +/- std errorbar and saves figure.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    medians = []\n",
    "    stds = []\n",
    "    for x_val, items in results.items():\n",
    "        ratios = []\n",
    "        # items may be list of floats or list of dicts or mixed\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and 'ratio' in it:\n",
    "                try:\n",
    "                    ratios.append(float(it['ratio']))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif isinstance(it, (float, int, np.floating, np.integer)):\n",
    "                ratios.append(float(it))\n",
    "            else:\n",
    "                # maybe nested dict with ratio inside\n",
    "                if isinstance(it, dict):\n",
    "                    for v in it.values():\n",
    "                        if isinstance(v, (float, int, np.floating, np.integer)):\n",
    "                            ratios.append(float(v))\n",
    "                            break\n",
    "        if len(ratios) == 0:\n",
    "            # skip if nothing found\n",
    "            print(f\"âš ï¸  No ratio entries found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "        xs.append(float(x_val))\n",
    "        medians.append(np.median(ratios))\n",
    "        stds.append(np.std(ratios))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"âš ï¸ No ratio data found in results. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    medians = np.array(medians)[order]\n",
    "    stds = np.array(stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, medians, yerr=stds, fmt='o-', capsize=5, ecolor='gray', elinewidth=1.5)\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(f\"Cumulative explained ratio (Î±={alpha})\")\n",
    "    plt.title(\"Model Complexity vs. Curvature Concentration\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/cumulative_explained_ratio_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        # ensure directory exists\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved cumulative ratio plot â†’ {save_path}\")\n",
    "\n",
    "def plot_logdet_vs_tracebound(results, save_path=None, clip_eps=1e-8):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ items ] }\n",
    "    Each item can be:\n",
    "      - a dict containing 'eigenvalues' (or 'eigvals', 'H', etc.)\n",
    "      - a direct np.ndarray of eigenvalues\n",
    "      - nested lists/dicts\n",
    "    This computes per-trial:\n",
    "      logdet = sum(log(clipped_eigvals))\n",
    "      trace_bound = d * log(mean(clipped_eigvals))\n",
    "    Then plots median +/- std across trials for each x (log n_params).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    logdet_medians = []\n",
    "    logdet_stds = []\n",
    "    trace_medians = []\n",
    "    trace_stds = []\n",
    "\n",
    "    for x_val, items in results.items():\n",
    "        # collect all eig arrays found under this x_val\n",
    "        eig_lists = []\n",
    "        for it in items:\n",
    "            arr = _safe_extract_eigvals(it)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            # convert to numpy and flatten\n",
    "            try:\n",
    "                arr = np.array(arr, dtype=float).flatten()\n",
    "            except Exception:\n",
    "                continue\n",
    "            eig_lists.append(arr)\n",
    "\n",
    "        if len(eig_lists) == 0:\n",
    "            print(f\"âš ï¸ No eigenvalue arrays found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        logdets = []\n",
    "        tracebounds = []\n",
    "        for eig in eig_lists:\n",
    "            # sanitize values\n",
    "            eig = eig[np.isfinite(eig)]\n",
    "            if eig.size == 0:\n",
    "                continue\n",
    "            eig = np.clip(eig, clip_eps, None)  # avoid non-positive\n",
    "            d = eig.size\n",
    "            if d == 0:\n",
    "                continue\n",
    "            logdet = np.sum(np.log(eig))\n",
    "            tracebound = d * np.log(np.sum(eig) / d)\n",
    "            logdets.append(logdet)\n",
    "            tracebounds.append(tracebound)\n",
    "\n",
    "        if len(logdets) == 0:\n",
    "            print(f\"âš ï¸ After sanitization no valid eigvals for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        xs.append(float(x_val))\n",
    "        logdet_medians.append(np.median(logdets))\n",
    "        logdet_stds.append(np.std(logdets))\n",
    "        trace_medians.append(np.median(tracebounds))\n",
    "        trace_stds.append(np.std(tracebounds))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"âš ï¸ No valid data to plot for logdet vs tracebound.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    logdet_medians = np.array(logdet_medians)[order]\n",
    "    logdet_stds = np.array(logdet_stds)[order]\n",
    "    trace_medians = np.array(trace_medians)[order]\n",
    "    trace_stds = np.array(trace_stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, logdet_medians, yerr=logdet_stds, fmt='o-', label='log|H| (âˆ‘ log Î»_i)')\n",
    "    plt.errorbar(xs, trace_medians, yerr=trace_stds, fmt='s--', label='trace bound (dÂ·log(tr/d))')\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Log-determinant vs. Trace Bound (Curvature Scale)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/logdet_vs_tracebound_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved logdet/trace plot â†’ {save_path}\")\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "plot_logdet_vs_tracebound(results, save_path='logdet_vs_tracebound.png')\n",
    "plot_cumulative_explained_ratio(results, alpha=0.9, save_path='cumulative_explained_ratio.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730900c",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684ae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing model with 650 parameters\n",
      "\n",
      "Analyzing model with 4810 parameters\n",
      "\n",
      "Analyzing model with 17226 parameters\n",
      "\n",
      "Analyzing model with 26122 parameters\n",
      "\n",
      "Analyzing model with 44170 parameters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_set) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m256\u001b[39m) :\n\u001b[0;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m train_set[\u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m i][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 136\u001b[0m     Values\u001b[38;5;241m.\u001b[39mappend([\u001b[43mDoptScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHessian\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem(), AoptScore(model, x, Hessian)\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[0;32m    137\u001b[0m Values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Values)\n\u001b[0;32m    138\u001b[0m Values \u001b[38;5;241m=\u001b[39m Values[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(Values)\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[1;32mIn[182], line 54\u001b[0m, in \u001b[0;36mDoptScore\u001b[1;34m(model, x, Hessian)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mDoptScore\u001b[39m(model, x, Hessian) :\n\u001b[1;32m---> 54\u001b[0m     U \u001b[38;5;241m=\u001b[39m \u001b[43mlow_rank_updated_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     Hinv_U \u001b[38;5;241m=\u001b[39m U \u001b[38;5;241m/\u001b[39m (Hessian\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m     56\u001b[0m     result \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Hinv_U\n",
      "Cell \u001b[1;32mIn[182], line 48\u001b[0m, in \u001b[0;36mlow_rank_updated_part\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlow_rank_updated_part\u001b[39m(model, x) :\n\u001b[1;32m---> 48\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_outcome_hessian_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     J \u001b[38;5;241m=\u001b[39m fast_jacobian(model,x)\n\u001b[0;32m     50\u001b[0m     H_sqrt \u001b[38;5;241m=\u001b[39m symmetric_matrix_sqrt(H)\n",
      "Cell \u001b[1;32mIn[182], line 20\u001b[0m, in \u001b[0;36mcompute_outcome_hessian_from_model\u001b[1;34m(model, inputs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_outcome_hessian_from_model\u001b[39m(model,inputs):\n\u001b[1;32m---> 20\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(z, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m     H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(p) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mouter(p,p)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:160\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\doyoung_laplace\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:341\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 2D or 3D input (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "def fast_jacobian(model, x):\n",
    "    model.zero_grad()\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)  # (1, input_dim)\n",
    "\n",
    "    y = model(x)  # (batch, output_dim)\n",
    "    y_dim = y.shape[-1]\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    J = []\n",
    "    for k in range(y_dim):  # ê° output dimì— ëŒ€í•´\n",
    "        grads = torch.autograd.grad(y[0, k], params, retain_graph=True)\n",
    "        grad_flat = torch.cat([g.reshape(-1) for g in grads])\n",
    "        J.append(grad_flat)\n",
    "    J = torch.stack(J, dim=0)  # (output_dim, n_params)\n",
    "    return J\n",
    "\n",
    "def compute_outcome_hessian_from_model(model,inputs):\n",
    "    z = model(inputs)\n",
    "    p = torch.softmax(z, dim=-1)\n",
    "    H = torch.diag(p) - torch.outer(p,p)\n",
    "    return H\n",
    "\n",
    "def symmetric_matrix_sqrt(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A: (n,n) ë˜ëŠ” (batch, n, n) - ì‹¤ìˆ˜ ëŒ€ì¹­ ê°€ì •\n",
    "    returns: A_sqrt with same shape (real)\n",
    "    \"\"\"\n",
    "    # batched ì²˜ë¦¬ í—ˆìš©\n",
    "    single = (A.dim() == 2)\n",
    "    if single:\n",
    "        A = A.unsqueeze(0)  # (1, n, n)\n",
    "\n",
    "    # eigen-decomposition (Hermitian ì „ìš©, ì•ˆì •ì )\n",
    "    w, v = torch.linalg.eigh(A)  # w: (batch,n), v: (batch,n,n)\n",
    "    # numerical stability: clamp eigenvalues\n",
    "    w_clamped = torch.clamp(w, min=eps)\n",
    "    w_sqrt = torch.sqrt(w_clamped)\n",
    "    # ìž¬êµ¬ì„±: V diag(sqrt(w)) V^T\n",
    "    A_sqrt = (v * w_sqrt.unsqueeze(-2)) @ v.transpose(-2, -1)\n",
    "\n",
    "    if single:\n",
    "        return A_sqrt[0]\n",
    "    return A_sqrt\n",
    "\n",
    "def low_rank_updated_part(model, x) :\n",
    "    H = compute_outcome_hessian_from_model(model,x)\n",
    "    J = fast_jacobian(model,x)\n",
    "    H_sqrt = symmetric_matrix_sqrt(H)\n",
    "    return J.T@H_sqrt\n",
    "\n",
    "def DoptScore(model, x, Hessian) :\n",
    "    U = low_rank_updated_part(model,x)\n",
    "    Hinv_U = U / (Hessian.unsqueeze(1) + 1e-10)\n",
    "    result = U.T @ Hinv_U\n",
    "    return torch.logdet(torch.eye(U.shape[1]) + result)\n",
    "\n",
    "def AoptScore(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute the A-optimality *reduction* score Delta =\n",
    "      tr( H^{-1} U (I + U^T H^{-1} U)^{-1} U^T H^{-1} )\n",
    "    where H is diagonal with entries H_diag (shape: (n_params,)) and\n",
    "    U has shape (n_params, d_out).\n",
    "\n",
    "    Returns: scalar Delta (torch scalar).  Higher -> better (bigger reduction in tr(Lambda^{-1})).\n",
    "    \"\"\"\n",
    "    H_diag = Hessian\n",
    "    U = low_rank_updated_part(model,x)\n",
    "    # U: (n_params, d_out)\n",
    "    # H_diag: (n_params,)\n",
    "    # 1) H^{-1} element\n",
    "    Hinv = 1.0 / (H_diag + eps)                 # (n_params,)\n",
    "\n",
    "    # 2) C = H^{-1} U  (elementwise multiply rows)\n",
    "    C = Hinv.unsqueeze(1) * U                  # (n_params, d_out)\n",
    "\n",
    "    # 3) A = I + U^T C  (d_out x d_out)\n",
    "    A = torch.eye(U.shape[1], device=U.device) + (U.T @ C)\n",
    "    A = A + eps * torch.eye(A.shape[0], device=A.device)  # stabilize\n",
    "\n",
    "    # 4) S = C^T C\n",
    "    S = C.T @ C                                 # (d_out, d_out)\n",
    "\n",
    "    # 5) solve A X = S  -> X = A^{-1} S\n",
    "    # Use torch.linalg.solve for numerical stability\n",
    "    X = torch.linalg.solve(A, S)                # (d_out, d_out)\n",
    "\n",
    "    # 6) Delta = trace(X)\n",
    "    Delta = torch.trace(X)\n",
    "    return Delta\n",
    "\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "Data = []\n",
    "for i in range(len(models)) :\n",
    "    model = models[i]\n",
    "   \n",
    "    n_params = count_parameters(model)\n",
    "    \n",
    "    print(f\"\\nAnalyzing model with {n_params} parameters\")\n",
    "    x_val = np.log(n_params)\n",
    "\n",
    "    torch.manual_seed(repeat)\n",
    "    np.random.seed(repeat)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    indices = list(range(len(train_set)))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:256]\n",
    "    train_subset = Subset(train_set, subset_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "    n_epochs_monitor = 30\n",
    "    for epoch in range(n_epochs_monitor):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    la.fit(train_loader)\n",
    "    la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "    Hessian = la.H\n",
    "    Values = []\n",
    "\n",
    "    for i in range(len(train_set) - 256) :\n",
    "        x = train_set[256 + i][0]\n",
    "        Values.append([DoptScore(model, x, Hessian).item(), AoptScore(model, x, Hessian).item()])\n",
    "    Values = np.array(Values)\n",
    "    Values = Values[~np.isnan(Values).any(axis=1)]\n",
    "    \n",
    "    D_scores = Values[:, 0]\n",
    "    A_scores = Values[:, 1]\n",
    "\n",
    "    # Pearson (ì„ í˜• ê´€ê³„)\n",
    "    pearson_corr, _ = pearsonr(D_scores, A_scores)\n",
    "\n",
    "    # Spearman (ìˆœìœ„ ê´€ê³„)\n",
    "    spearman_corr, _ = spearmanr(D_scores, A_scores)\n",
    "    Data.append([n_params, pearson_corr, spearman_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a25353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1091, 2)\n"
     ]
    }
   ],
   "source": [
    "def fast_jacobian(model, x):\n",
    "    model.zero_grad()\n",
    "    # ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)  # (1, input_dim)\n",
    "    b = x.shape[0]\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # precompute param sizes to flatten consistently\n",
    "    param_numels = [p.numel() for p in params]\n",
    "\n",
    "    J_batch = []\n",
    "    for i in range(b):\n",
    "        xi = x[i : i + 1]  # keep batch dim for forward\n",
    "        yi = model(xi)  # (1, d_out)\n",
    "        d_out = yi.shape[-1]\n",
    "\n",
    "        # compute grads for this sample: result (d_out, n_params)\n",
    "        J_i_rows = []\n",
    "        for k in range(d_out):\n",
    "            grads = torch.autograd.grad(yi[0, k], params, retain_graph=True)\n",
    "            grad_flat = torch.cat([g.reshape(-1) for g in grads])\n",
    "            J_i_rows.append(grad_flat)\n",
    "        J_i = torch.stack(J_i_rows, dim=0)  # (d_out, n_params)\n",
    "        J_batch.append(J_i)\n",
    "\n",
    "    J_batch = torch.stack(J_batch, dim=0)  # (b, d_out, n_params)\n",
    "    return J_batch\n",
    "# ...existing code...\n",
    "def compute_outcome_hessian_from_model(model, inputs):\n",
    "    # inputs: (d,) or (b, d)\n",
    "    if inputs.ndim == 1:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    z = model(inputs)  # (b, d_out)\n",
    "    p = torch.softmax(z, dim=-1)  # (b, d_out)\n",
    "    # diag_embed builds (b, d_out, d_out)\n",
    "    H = torch.diag_embed(p) - p.unsqueeze(2) * p.unsqueeze(1)  # (b, d_out, d_out)\n",
    "    return H\n",
    "# ...existing code...\n",
    "def symmetric_matrix_sqrt(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A: (n,n) or (batch, n, n)\n",
    "    returns: A_sqrt with same shape\n",
    "    \"\"\"\n",
    "    single = (A.dim() == 2)\n",
    "    if single:\n",
    "        A = A.unsqueeze(0)\n",
    "    w, v = torch.linalg.eigh(A)\n",
    "    w_clamped = torch.clamp(w, min=eps)\n",
    "    w_sqrt = torch.sqrt(w_clamped)\n",
    "    A_sqrt = (v * w_sqrt.unsqueeze(-2)) @ v.transpose(-2, -1)\n",
    "    if single:\n",
    "        return A_sqrt[0]\n",
    "    return A_sqrt\n",
    "# ...existing code...\n",
    "# ...existing code...\n",
    "def low_rank_updated_part(model, x, return_batch: bool = False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - if return_batch=True: U_batch of shape (b, n_params, d_out)\n",
    "      - else: U_all of shape (n_params, b * d_out)  (backward-compatible)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    H = compute_outcome_hessian_from_model(model, x)    # (b, d_out, d_out)\n",
    "    J_batch = fast_jacobian(model, x)                   # (b, d_out, n_params)\n",
    "    H_sqrt = symmetric_matrix_sqrt(H)                   # (b, d_out, d_out)\n",
    "\n",
    "    # J_batch: (b, d_out, n_params) -> transpose -> (b, n_params, d_out)\n",
    "    Jt = J_batch.transpose(1, 2)\n",
    "    U_batch = torch.matmul(Jt, H_sqrt)                  # (b, n_params, d_out)\n",
    "\n",
    "    if return_batch:\n",
    "        return U_batch\n",
    "    b, n_params, d_out = U_batch.shape\n",
    "    U_all = U_batch.permute(1, 0, 2).reshape(n_params, b * d_out)\n",
    "    return U_all\n",
    "\n",
    "def DoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute D-opt score per input sample.\n",
    "    - x: (input_dim,) or (b, input_dim)\n",
    "    - Hessian: torch tensor (n_params,) (diagonal)\n",
    "    Returns: torch tensor shape (b,) with per-sample log-determinant scores\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        # use slogdet for stability\n",
    "        sign, ld = torch.linalg.slogdet(A)\n",
    "        # if numeric issue (sign <=0) return nan for that sample\n",
    "        scores.append(ld if sign > 0 else torch.tensor(float('nan'), device=A.device))\n",
    "    return torch.stack(scores)  # (b,)\n",
    "\n",
    "def AoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute A-opt reduction per input sample.\n",
    "    Returns: torch tensor shape (b,) with per-sample Delta values.\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    deltas = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        A = A + eps * torch.eye(A.shape[0], device=A.device)\n",
    "        S = C.T @ C                          # (d_out, d_out)\n",
    "        X = torch.linalg.solve(A, S)         # (d_out, d_out)\n",
    "        deltas.append(torch.trace(X))\n",
    "    return torch.stack(deltas)  # (b,)\n",
    "# ...existing code...\n",
    "\n",
    "models = get_models()\n",
    "model = models[0]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "indices = list(range(len(train_set)))\n",
    "random.shuffle(indices)\n",
    "subset_indices = indices[:256]\n",
    "train_subset = Subset(train_set, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "n_epochs_monitor = 30\n",
    "for epoch in range(n_epochs_monitor):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "la.fit(train_loader)\n",
    "Hessian = la.H\n",
    "Values = []\n",
    "x = train_set[0]\n",
    "for i in range(len(train_set) - 256) :\n",
    "    x = train_set[256 + i][0]\n",
    "    Values.append([DoptScore(model, x, Hessian).item(), AoptScore(model, x, Hessian).item()])\n",
    "Values = np.array(Values)\n",
    "print(Values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10b2ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 5.1648e-01, 1.6782e+00, 1.7296e+00, 1.9861e+00, 1.7735e+00,\n",
       "        7.1693e-01, 2.0605e-01, 9.6145e-03, 1.2395e+00, 1.8048e+00, 2.1838e+00,\n",
       "        2.3190e+00, 2.0791e+00, 1.3869e+00, 2.8057e-01, 3.5965e-03, 1.7609e+00,\n",
       "        1.8001e+00, 2.3279e+00, 2.1819e+00, 2.4254e+00, 2.8138e+00, 5.7052e-01,\n",
       "        1.2367e-01, 2.5486e+00, 2.1170e+00, 2.1771e+00, 3.8973e+00, 2.1383e+00,\n",
       "        2.8540e+00, 5.4399e-03, 0.0000e+00, 2.7391e+00, 2.6097e+00, 2.8426e+00,\n",
       "        4.7713e+00, 1.7002e+00, 3.4033e+00, 0.0000e+00, 3.0161e-02, 3.1463e+00,\n",
       "        2.9181e+00, 2.1965e+00, 2.5412e+00, 2.0138e+00, 2.4359e+00, 1.4183e+00,\n",
       "        3.6061e-03, 2.2262e+00, 2.2631e+00, 2.3598e+00, 2.3566e+00, 2.0104e+00,\n",
       "        1.5574e+00, 1.4919e-01, 1.7969e-03, 2.8449e-01, 1.4028e+00, 1.8352e+00,\n",
       "        1.4210e+00, 1.5939e+00, 9.4818e-01, 1.0684e-01, 0.0000e+00, 1.2891e+00,\n",
       "        4.5795e+00, 1.2898e+01, 4.2964e+00, 6.0981e+00, 7.0389e+00, 3.5245e-01,\n",
       "        2.0801e-02, 3.8520e+00, 8.1616e+00, 4.9812e+00, 4.9291e+00, 4.7954e+00,\n",
       "        8.6801e+00, 2.6486e-01, 7.7809e-03, 6.7264e+00, 5.1949e+00, 8.0100e+00,\n",
       "        6.6151e+00, 4.8333e+00, 6.2483e+00, 8.3034e-02, 8.3572e-02, 3.8626e+00,\n",
       "        4.3730e+00, 5.4007e+00, 3.9344e+00, 5.9942e+00, 2.8131e+00, 1.1682e-02,\n",
       "        0.0000e+00, 2.9254e+00, 4.2796e+00, 4.8154e+00, 3.4348e+00, 7.0528e+00,\n",
       "        4.3063e+00, 0.0000e+00, 3.2340e-02, 2.7923e+00, 4.8076e+00, 5.9852e+00,\n",
       "        4.6177e+00, 6.2370e+00, 3.6176e+00, 1.1406e+00, 7.5760e-03, 1.3055e+00,\n",
       "        5.0711e+00, 7.6931e+00, 4.7618e+00, 5.7814e+00, 4.8300e+00, 1.3315e+01,\n",
       "        3.8876e-03, 7.3090e-01, 4.7215e+00, 1.2703e+01, 3.7253e+00, 5.4679e+00,\n",
       "        5.7173e+00, 4.5969e+00, 0.0000e+00, 3.4646e+00, 3.3754e+00, 3.4323e+00,\n",
       "        5.8058e+00, 2.6962e+00, 8.7638e-01, 5.9108e-02, 1.1780e-02, 3.1912e+00,\n",
       "        3.1672e+00, 3.3374e+00, 2.4350e+00, 3.3436e+00, 3.8094e+00, 4.1474e-01,\n",
       "        4.4067e-03, 3.5008e+00, 2.6724e+00, 2.9494e+00, 3.7113e+00, 2.8484e+00,\n",
       "        3.3183e+00, 4.5126e-02, 4.1937e-02, 2.0003e+00, 4.1471e+00, 2.9150e+00,\n",
       "        1.6435e+00, 2.9567e+00, 1.3861e+00, 6.6155e-03, 0.0000e+00, 1.2651e+00,\n",
       "        2.5262e+00, 2.9779e+00, 1.9150e+00, 3.8935e+00, 2.0448e+00, 0.0000e+00,\n",
       "        1.5877e-02, 2.4062e+00, 2.6668e+00, 3.1384e+00, 3.0185e+00, 3.9622e+00,\n",
       "        2.5498e+00, 2.0526e+00, 4.2887e-03, 3.8596e+00, 2.8623e+00, 2.7855e+00,\n",
       "        2.5768e+00, 2.1641e+00, 5.1040e+00, 1.4759e+01, 2.2017e-03, 3.3009e+00,\n",
       "        3.4511e+00, 2.9476e+00, 2.2014e+00, 3.6020e+00, 7.1483e+00, 4.5692e+00,\n",
       "        0.0000e+00, 6.6689e+00, 3.5657e+00, 2.4953e+00, 3.2972e+00, 3.2809e+00,\n",
       "        9.8046e-01, 7.3693e-02, 1.5745e-02, 5.6709e+00, 2.5059e+00, 4.1643e+00,\n",
       "        3.0559e+00, 3.8309e+00, 2.4528e+00, 1.3789e-01, 5.8898e-03, 4.8602e+00,\n",
       "        3.7914e+00, 3.8878e+00, 4.4666e+00, 3.6248e+00, 1.2837e+00, 4.9847e-02,\n",
       "        3.6768e-03, 2.8783e+00, 4.4130e+00, 3.6001e+00, 2.2310e+00, 3.5766e+00,\n",
       "        1.7735e+00, 8.8413e-03, 0.0000e+00, 1.7305e+00, 4.0600e+00, 4.5915e+00,\n",
       "        3.7423e+00, 3.5438e+00, 4.5298e+00, 0.0000e+00, 1.8561e-02, 2.0510e+00,\n",
       "        3.6996e+00, 4.1721e+00, 4.7796e+00, 3.0449e+00, 6.0660e+00, 2.3964e-01,\n",
       "        5.7304e-03, 3.7137e+00, 3.8671e+00, 4.6795e+00, 3.3245e+00, 3.5558e+00,\n",
       "        5.2654e+00, 1.1679e+00, 2.9427e-03, 2.8965e+00, 3.4435e+00, 2.5538e+00,\n",
       "        2.5649e+00, 3.6997e+00, 4.0782e+00, 1.2960e+00, 0.0000e+00, 5.3664e-01,\n",
       "        2.2582e+00, 5.3216e+00, 2.1107e+00, 2.2284e+00, 1.4939e+00, 1.3034e-01,\n",
       "        9.1741e-03, 9.2077e-01, 3.9434e+00, 1.9124e+00, 2.5423e+00, 2.3383e+00,\n",
       "        2.5100e+00, 2.2969e-01, 3.4318e-03, 1.1264e+00, 2.0663e+00, 2.6320e+00,\n",
       "        2.0908e+00, 2.2223e+00, 4.5271e+00, 3.5721e+00, 1.8312e+00, 2.1411e+00,\n",
       "        1.9567e+00, 1.6843e+00, 2.5500e+00, 2.4207e+00, 2.7753e+00, 5.2049e-03,\n",
       "        0.0000e+00, 4.1628e+00, 2.7001e+00, 2.1017e+00, 2.4263e+00, 2.2888e+00,\n",
       "        2.7463e+00, 0.0000e+00, 1.8785e-01, 4.3050e+00, 2.2876e+00, 1.9623e+00,\n",
       "        2.3740e+00, 2.1246e+00, 1.3056e+00, 2.5132e-01, 3.4777e-03, 2.7150e+00,\n",
       "        2.7887e+00, 3.1722e+00, 2.6354e+00, 2.2153e+00, 1.4149e+00, 1.4513e-01,\n",
       "        1.7146e-03, 3.5555e-01, 2.0615e+00, 5.3520e+00, 1.7090e+00, 1.9126e+00,\n",
       "        8.8213e-01, 1.1092e-01, 0.0000e+00, 3.2148e+00, 3.3024e+00, 1.7606e+00,\n",
       "        3.9495e+00, 4.3109e+00, 5.0474e+00, 1.1226e+00, 1.3756e-02, 4.2838e+00,\n",
       "        2.3610e+00, 3.1123e+00, 3.1815e+00, 3.0451e+00, 7.2671e+00, 2.1049e+00,\n",
       "        5.1458e-03, 5.3166e+00, 2.2633e+00, 3.2723e+00, 3.0750e+00, 3.4000e+00,\n",
       "        2.7437e+00, 4.1963e-01, 1.1980e+00, 5.3446e+00, 3.2725e+00, 2.9456e+00,\n",
       "        2.6356e+00, 3.3620e+00, 1.9995e+00, 7.7245e-03, 0.0000e+00, 1.9942e+00,\n",
       "        2.6721e+00, 2.7015e+00, 3.6793e+00, 3.4602e+00, 3.1747e+00, 0.0000e+00,\n",
       "        1.7253e-02, 1.2035e+00, 3.5191e+00, 3.0634e+00, 3.3983e+00, 3.8231e+00,\n",
       "        2.4643e+00, 1.3204e-01, 5.0067e-03, 1.8092e+00, 2.5782e+00, 3.2990e+00,\n",
       "        2.2624e+00, 3.8958e+00, 2.0245e+00, 2.2308e-01, 2.5710e-03, 2.1593e+00,\n",
       "        3.5342e+00, 1.9506e+00, 5.2426e+00, 3.0452e+00, 1.4049e+00, 1.3736e-01,\n",
       "        0.0000e+00, 4.7825e-01, 2.0021e+00, 1.8306e+00, 2.8949e+00, 1.9405e+00,\n",
       "        6.6306e-01, 4.3459e-02, 9.8324e-03, 1.0560e+00, 1.8590e+00, 1.6219e+00,\n",
       "        2.3145e+00, 2.7798e+00, 1.0671e+00, 5.7914e-02, 3.6780e-03, 1.1003e+00,\n",
       "        1.9612e+00, 2.2842e+00, 2.6209e+00, 3.0366e+00, 1.1030e+00, 1.5317e-01,\n",
       "        3.7218e-01, 2.8335e+00, 2.2181e+00, 2.1123e+00, 3.1126e+00, 2.2643e+00,\n",
       "        1.3880e+00, 5.5221e-03, 0.0000e+00, 2.5108e+00, 2.9694e+00, 2.1709e+00,\n",
       "        2.2358e+00, 2.4436e+00, 2.1432e+00, 0.0000e+00, 1.5056e-01, 1.9596e+00,\n",
       "        3.2383e+00, 2.2111e+00, 2.3839e+00, 1.8955e+00, 2.9586e+00, 2.8399e+00,\n",
       "        3.5809e-03, 1.1413e+00, 1.6947e+00, 2.2347e+00, 2.1905e+00, 1.9612e+00,\n",
       "        2.8898e+00, 1.2246e+00, 1.8376e-03, 5.1210e-01, 1.9654e+00, 1.6198e+00,\n",
       "        1.4949e+00, 1.9128e+00, 1.9563e+00, 1.7946e-01, 0.0000e+00, 8.6444e-01,\n",
       "        2.3635e+00, 2.8742e+00, 2.3835e+00, 4.1599e+00, 1.1510e+01, 3.0587e+01,\n",
       "        1.0741e-02, 1.4029e+00, 1.7141e+00, 2.2749e+00, 1.7817e+00, 2.4628e+00,\n",
       "        1.4774e+01, 2.2241e+01, 4.0180e-03, 2.0125e+00, 2.8761e+00, 2.5184e+00,\n",
       "        2.5332e+00, 2.1898e+00, 6.0724e+00, 8.2437e-01, 4.6189e-03, 1.6121e+00,\n",
       "        2.7676e+00, 2.5709e+00, 2.0725e+00, 3.0532e+00, 3.1407e+00, 6.0318e-03,\n",
       "        0.0000e+00, 1.4688e+00, 2.4391e+00, 2.6502e+00, 2.1142e+00, 2.9234e+00,\n",
       "        2.7492e+00, 0.0000e+00, 1.9116e-02, 1.4437e+00, 2.3328e+00, 2.6573e+00,\n",
       "        2.1915e+00, 3.5327e+00, 1.6645e+00, 1.3220e-01, 3.9100e-03, 7.5431e-01,\n",
       "        2.8946e+00, 2.3666e+00, 2.7855e+00, 4.4486e+00, 1.6120e+00, 1.4714e-01,\n",
       "        2.0075e-03, 7.6109e-01, 3.0607e+00, 3.1053e+00, 7.4440e+00, 3.2495e+00,\n",
       "        1.2359e+00, 1.2261e-01, 0.0000e+00, 3.3894e+00, 5.2858e+00, 6.5671e+00,\n",
       "        6.3142e+00, 5.6326e+00, 3.6055e+00, 2.0027e-01, 2.3954e-02, 2.8307e+00,\n",
       "        6.0971e+00, 6.8342e+00, 4.8825e+00, 5.0311e+00, 8.7290e+00, 1.2729e-01,\n",
       "        8.9606e-03, 5.0329e+00, 5.1554e+00, 5.9390e+00, 6.3713e+00, 5.0375e+00,\n",
       "        4.8672e+00, 1.4436e-01, 1.7961e-02, 4.6705e+00, 5.3524e+00, 5.3543e+00,\n",
       "        4.0601e+00, 5.7357e+00, 2.2520e+00, 1.3451e-02, 0.0000e+00, 2.4237e+00,\n",
       "        4.3251e+00, 4.4486e+00, 3.3032e+00, 7.7311e+00, 3.9860e+00, 0.0000e+00,\n",
       "        2.9176e-02, 2.7081e+00, 5.6470e+00, 6.0885e+00, 4.7867e+00, 6.7918e+00,\n",
       "        3.5290e+00, 6.8369e-01, 8.7181e-03, 4.0236e+00, 5.5259e+00, 6.3827e+00,\n",
       "        5.6355e+00, 5.1456e+00, 4.3355e+00, 1.1617e+00, 4.4770e-03, 2.0428e+00,\n",
       "        5.5029e+00, 6.2901e+00, 4.4421e+00, 5.1529e+00, 4.9085e+00, 3.0324e-01,\n",
       "        0.0000e+00, 4.5012e+00, 5.8588e+00, 8.2579e+00, 7.9144e+00, 9.6040e+00,\n",
       "        1.6330e+01, 3.0505e+01, 3.0273e-02, 8.0644e+00, 5.4417e+00, 8.5761e+00,\n",
       "        6.1325e+00, 6.8642e+00, 1.9301e+01, 2.1069e+01, 1.1324e-02, 1.1287e+01,\n",
       "        4.9638e+00, 7.6234e+00, 6.5427e+00, 7.5327e+00, 1.1606e+01, 2.8923e+00,\n",
       "        1.2675e-02, 6.1637e+00, 6.7654e+00, 6.9353e+00, 5.0006e+00, 8.4460e+00,\n",
       "        5.7159e+00, 1.6999e-02, 0.0000e+00, 3.0052e+00, 7.2133e+00, 8.2917e+00,\n",
       "        8.2559e+00, 6.5914e+00, 8.9226e+00, 0.0000e+00, 3.5395e-02, 2.2537e+00,\n",
       "        7.5415e+00, 7.9554e+00, 7.8787e+00, 7.2287e+00, 8.6950e+00, 3.4759e-01,\n",
       "        1.1018e-02, 2.9133e+00, 7.5313e+00, 9.0406e+00, 6.9485e+00, 7.8084e+00,\n",
       "        8.3163e+00, 1.1581e+00, 5.6579e-03, 1.8752e+00, 6.3165e+00, 8.9467e+00,\n",
       "        1.0051e+01, 8.7488e+00, 6.6470e+00, 5.6951e-01, 2.4187e+00, 5.2327e+00,\n",
       "        2.9635e+00, 3.9609e+00, 2.3079e+00, 3.4606e+00, 2.4735e+00, 2.7021e+00,\n",
       "        6.0260e+00, 7.6155e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d813ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18d4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30.8276,   7.5965,  13.0775,  29.7956,  17.8141,  14.9147,   0.9315,\n",
       "         34.6335,  35.1382,  30.7006, 111.0116,  39.4122,  54.6801,  27.3965,\n",
       "          5.9850,   3.6901,  16.8914,  15.9699,  39.8890,  12.4648,  29.4038,\n",
       "         36.0105,  16.6142,  86.4413,  10.0762,  72.9990,  23.9596,  17.8962,\n",
       "         13.5748,  36.0925,  17.4318,  15.2944,  60.7858,   4.1995,   2.7104,\n",
       "         61.6482,  26.7581,  59.8306,  25.8158,   2.6334,  35.6111,  21.3935,\n",
       "         18.4044], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_set[1:44][0]\n",
    "AoptScore_per_sample(model, x, Hessian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoung_laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
