{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79bae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Libraries'''\n",
    "import numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from laplace import Laplace\n",
    "import matplotlib.pyplot as plt\n",
    "import laplace\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bb46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001607C40BBE0>\n"
     ]
    }
   ],
   "source": [
    "'''Dataset Loader for Digits dataset'''\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data\n",
    "digits = load_digits()\n",
    "X, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)\n",
    "ytr_t = torch.from_numpy(ytr)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "train_set = TensorDataset(Xtr_t, ytr_t)\n",
    "print(train_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc5687",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a51551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, only_trainable=False):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    only_trainable=Trueì´ë©´ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ ì…‰ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    params = (p for p in model.parameters() if (not only_trainable) or p.requires_grad)\n",
    "    return sum(p.numel() for p in params)\n",
    "\n",
    "def get_models():\n",
    "    models = []\n",
    "\n",
    "    # 1. ê°€ìž¥ ë‹¨ìˆœí•œ ëª¨ë¸\n",
    "    models.append(nn.Linear(64, 10))\n",
    "\n",
    "    # 2. íŒŒë¼ë¯¸í„° 2ë°°: Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 3. ë” ê¹Šê²Œ: Linear + Linear + Linear\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    ))\n",
    "\n",
    "    # 4. ë” ê¹Šê³  ë„“ê²Œ\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 5. BatchNorm ì¶”ê°€\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ))\n",
    "\n",
    "    # 6. Dropout ì¶”ê°€\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 7. ë” ê¹Šê²Œ, ë” ë„“ê²Œ\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 8. ë” ë§Žì€ ë ˆì´ì–´ì™€ BatchNorm, Dropout\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 9. ë” ê¹Šê³  ë„“ê²Œ, í™œì„±í™” ë‹¤ì–‘í™”\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    # 10. ê°€ìž¥ í° ëª¨ë¸\n",
    "    models.append(nn.Sequential(\n",
    "        nn.Linear(64, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    ))\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate(loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return correct / total\n",
    "\n",
    "def cumulative_explained_ratio(val, alpha=0.9):\n",
    "    \"\"\"\n",
    "    val: list or np.array of eigenvalues (can be positive/negative)\n",
    "    alpha: target cumulative proportion (e.g., 0.9)\n",
    "    \n",
    "    Returns:\n",
    "        ratio (float): index/p where cumulative sum first exceeds alpha\n",
    "        idx (int): the actual index achieving it\n",
    "    \"\"\"\n",
    "    val = np.array(val, dtype=float)\n",
    "    # ìŒìˆ˜ ê°’ì€ curvature ì„¤ëª…ì— ê¸°ì—¬í•˜ì§€ ì•Šë„ë¡ ì œê±° (í•„ìš”ì‹œ ì˜µì…˜í™” ê°€ëŠ¥)\n",
    "    val = np.maximum(val, 0)\n",
    "    \n",
    "    if np.sum(val) == 0:\n",
    "        return 0.0, 0\n",
    "\n",
    "    sorted_vals = np.sort(val)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    cumvals = np.cumsum(sorted_vals)\n",
    "    total = cumvals[-1]\n",
    "    threshold = alpha * total\n",
    "\n",
    "    idx = np.searchsorted(cumvals, threshold)\n",
    "    ratio = (idx + 1) / len(val)\n",
    "    return ratio, idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 10\n",
    "alpha = 0.9\n",
    "results = {}\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"\\n=== Repetition {repeat+1}/{n_repeats} ===\")\n",
    "    \n",
    "    # ðŸ’¡ ë§¤ ë°˜ë³µë§ˆë‹¤ fresh initialization\n",
    "    models = get_models()\n",
    "\n",
    "    for model in models:\n",
    "        n_params = count_parameters(model)\n",
    "        x_val = np.log(n_params)\n",
    "\n",
    "        print(f\"\\nTraining model with {n_params} parameters (repeat {repeat+1})\")\n",
    "\n",
    "        torch.manual_seed(repeat)\n",
    "        np.random.seed(repeat)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "        train_eval_loader = DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "\n",
    "        n_epochs_monitor = 30\n",
    "        for epoch in range(n_epochs_monitor):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        # after training:\n",
    "        la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "        la.fit(train_loader)\n",
    "        val = la.H.cpu().numpy()\n",
    "\n",
    "        ratio, idx = cumulative_explained_ratio(val, alpha=alpha)\n",
    "\n",
    "        # ðŸ’¾ ratio + full eigenvalues ë‘˜ ë‹¤ ì €ìž¥\n",
    "        results.setdefault(x_val, []).append({\n",
    "            'ratio': ratio,\n",
    "            'eigenvalues': val\n",
    "        })\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved logdet/trace plot â†’ logdet_vs_tracebound.png\n",
      "âœ… Saved cumulative ratio plot â†’ cumulative_explained_ratio.png\n"
     ]
    }
   ],
   "source": [
    "def _safe_extract_eigvals(obj):\n",
    "    \"\"\"\n",
    "    Try to find an eigenvalue array (np.ndarray or list-like) inside obj.\n",
    "    Return a 1D numpy array if found, else return None.\n",
    "    Handles: np.ndarray, list/tuple of arrays, dict with keys like 'eigenvalues','eigvals','H','values',\n",
    "    and nested structures.\n",
    "    \"\"\"\n",
    "    # direct numpy array\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.flatten()\n",
    "\n",
    "    # numpy scalar or Python scalar -> not an eigenvalue array\n",
    "    if isinstance(obj, (np.floating, np.integer, float, int, np.float64, np.int64)):\n",
    "        return None\n",
    "\n",
    "    # list/tuple -> try each element\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            arr = _safe_extract_eigvals(item)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # dict -> try common keys first, then recurse over values\n",
    "    if isinstance(obj, dict):\n",
    "        # common possible keys that hold eigenvalues\n",
    "        preferred_keys = ['eigenvalues', 'eigvals', 'eigvals_', 'H', 'values', 'vals', 'eigs', 'eigen']\n",
    "        for k in preferred_keys:\n",
    "            if k in obj:\n",
    "                arr = _safe_extract_eigvals(obj[k])\n",
    "                if arr is not None:\n",
    "                    return arr\n",
    "\n",
    "        # if none of preferred keys, recurse values (but skip scalar values)\n",
    "        for v in obj.values():\n",
    "            arr = _safe_extract_eigvals(v)\n",
    "            if arr is not None:\n",
    "                return arr\n",
    "        return None\n",
    "\n",
    "    # other types (pandas, torch tensor) - handle torch tensors\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.detach().cpu().numpy().flatten()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: not recognized\n",
    "    return None\n",
    "\n",
    "def plot_cumulative_explained_ratio(results, alpha=0.9, save_path=None):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ either floats OR dicts with 'ratio' key ] }\n",
    "    Draws median +/- std errorbar and saves figure.\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    medians = []\n",
    "    stds = []\n",
    "    for x_val, items in results.items():\n",
    "        ratios = []\n",
    "        # items may be list of floats or list of dicts or mixed\n",
    "        for it in items:\n",
    "            if isinstance(it, dict) and 'ratio' in it:\n",
    "                try:\n",
    "                    ratios.append(float(it['ratio']))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            elif isinstance(it, (float, int, np.floating, np.integer)):\n",
    "                ratios.append(float(it))\n",
    "            else:\n",
    "                # maybe nested dict with ratio inside\n",
    "                if isinstance(it, dict):\n",
    "                    for v in it.values():\n",
    "                        if isinstance(v, (float, int, np.floating, np.integer)):\n",
    "                            ratios.append(float(v))\n",
    "                            break\n",
    "        if len(ratios) == 0:\n",
    "            # skip if nothing found\n",
    "            print(f\"âš ï¸  No ratio entries found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "        xs.append(float(x_val))\n",
    "        medians.append(np.median(ratios))\n",
    "        stds.append(np.std(ratios))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"âš ï¸ No ratio data found in results. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    medians = np.array(medians)[order]\n",
    "    stds = np.array(stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, medians, yerr=stds, fmt='o-', capsize=5, ecolor='gray', elinewidth=1.5)\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(f\"Cumulative explained ratio (Î±={alpha})\")\n",
    "    plt.title(\"Model Complexity vs. Curvature Concentration\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/cumulative_explained_ratio_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        # ensure directory exists\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved cumulative ratio plot â†’ {save_path}\")\n",
    "\n",
    "def plot_logdet_vs_tracebound(results, save_path=None, clip_eps=1e-8):\n",
    "    \"\"\"\n",
    "    results: dict { log(n_params): [ items ] }\n",
    "    Each item can be:\n",
    "      - a dict containing 'eigenvalues' (or 'eigvals', 'H', etc.)\n",
    "      - a direct np.ndarray of eigenvalues\n",
    "      - nested lists/dicts\n",
    "    This computes per-trial:\n",
    "      logdet = sum(log(clipped_eigvals))\n",
    "      trace_bound = d * log(mean(clipped_eigvals))\n",
    "    Then plots median +/- std across trials for each x (log n_params).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    logdet_medians = []\n",
    "    logdet_stds = []\n",
    "    trace_medians = []\n",
    "    trace_stds = []\n",
    "\n",
    "    for x_val, items in results.items():\n",
    "        # collect all eig arrays found under this x_val\n",
    "        eig_lists = []\n",
    "        for it in items:\n",
    "            arr = _safe_extract_eigvals(it)\n",
    "            if arr is None:\n",
    "                continue\n",
    "            # convert to numpy and flatten\n",
    "            try:\n",
    "                arr = np.array(arr, dtype=float).flatten()\n",
    "            except Exception:\n",
    "                continue\n",
    "            eig_lists.append(arr)\n",
    "\n",
    "        if len(eig_lists) == 0:\n",
    "            print(f\"âš ï¸ No eigenvalue arrays found for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        logdets = []\n",
    "        tracebounds = []\n",
    "        for eig in eig_lists:\n",
    "            # sanitize values\n",
    "            eig = eig[np.isfinite(eig)]\n",
    "            if eig.size == 0:\n",
    "                continue\n",
    "            eig = np.clip(eig, clip_eps, None)  # avoid non-positive\n",
    "            d = eig.size\n",
    "            if d == 0:\n",
    "                continue\n",
    "            logdet = np.sum(np.log(eig))\n",
    "            tracebound = d * np.log(np.sum(eig) / d)\n",
    "            logdets.append(logdet)\n",
    "            tracebounds.append(tracebound)\n",
    "\n",
    "        if len(logdets) == 0:\n",
    "            print(f\"âš ï¸ After sanitization no valid eigvals for x={x_val}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        xs.append(float(x_val))\n",
    "        logdet_medians.append(np.median(logdets))\n",
    "        logdet_stds.append(np.std(logdets))\n",
    "        trace_medians.append(np.median(tracebounds))\n",
    "        trace_stds.append(np.std(tracebounds))\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        print(\"âš ï¸ No valid data to plot for logdet vs tracebound.\")\n",
    "        return\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    order = np.argsort(xs)\n",
    "    xs = xs[order]\n",
    "    logdet_medians = np.array(logdet_medians)[order]\n",
    "    logdet_stds = np.array(logdet_stds)[order]\n",
    "    trace_medians = np.array(trace_medians)[order]\n",
    "    trace_stds = np.array(trace_stds)[order]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(xs, logdet_medians, yerr=logdet_stds, fmt='o-', label='log|H| (âˆ‘ log Î»_i)')\n",
    "    plt.errorbar(xs, trace_medians, yerr=trace_stds, fmt='s--', label='trace bound (dÂ·log(tr/d))')\n",
    "    plt.xlabel(\"log(Number of parameters)\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Log-determinant vs. Trace Bound (Curvature Scale)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(\"figs\", exist_ok=True)\n",
    "    if save_path is None:\n",
    "        save_path = f\"figs/logdet_vs_tracebound_{time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    else:\n",
    "        d = os.path.dirname(save_path)\n",
    "        if d:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"âœ… Saved logdet/trace plot â†’ {save_path}\")\n",
    "\n",
    "with open(\"results_curvature_concentration.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "plot_logdet_vs_tracebound(results, save_path='logdet_vs_tracebound.png')\n",
    "plot_cumulative_explained_ratio(results, alpha=0.9, save_path='cumulative_explained_ratio.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730900c",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabeled_train_loader = DataLoader(labeled_train_set, batch_size=32, shuffle=True)\\nunlabeled_train_loader = DataLoader(unlabeled_train_set, batch_size=32, shuffle=True)\\ntest_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Data ì¤€ë¹„ (ê¸°ì¡´ Xtr_t, ytr_t ì‚¬ìš©)\n",
    "digits = load_digits()\n",
    "x, y = digits.data.astype(np.float32), digits.target.astype(np.int64)\n",
    "Xtr, Xte, ytr, yte = train_test_split(x, y, test_size=0.25, stratify=y, random_state=42)\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr, Xte = scaler.transform(Xtr), scaler.transform(Xte)\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr)          # (N_train, D)\n",
    "ytr_t = torch.from_numpy(ytr).long()   # (N_train,)\n",
    "Xte_t = torch.from_numpy(Xte)\n",
    "yte_t = torch.from_numpy(yte)\n",
    "\n",
    "num_pretrain = 30\n",
    "indices = list(range(len(Xtr_t)))\n",
    "random.shuffle(indices)\n",
    "labeled_indices = indices[:num_pretrain]       # ì‹¤ì œ dataset ì¸ë±ìŠ¤ë“¤\n",
    "unlabeled_indices = indices[num_pretrain:]\n",
    "\n",
    "# ê¸°ë³¸ ì „ì²´ í…ì„œë°ì´í„°ì…‹ í•˜ë‚˜ ìƒì„±\n",
    "full_train_dataset = TensorDataset(Xtr_t, ytr_t)\n",
    "\n",
    "# Subsetìœ¼ë¡œ ë¼ë²¨/ì–¸ë¼ë²¨ë“œ ê´€ë¦¬ (TensorDatasetì— ì§ì ‘ numpy ë„£ì§€ ì•ŠìŒ)\n",
    "train_set_Labeled = Subset(full_train_dataset, labeled_indices)\n",
    "train_set_Unlabeled = Subset(full_train_dataset, unlabeled_indices)\n",
    "test_set = TensorDataset(Xte_t, yte_t)\n",
    "\n",
    "\n",
    "'''\n",
    "labeled_train_loader = DataLoader(labeled_train_set, batch_size=32, shuffle=True)\n",
    "unlabeled_train_loader = DataLoader(unlabeled_train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=256, shuffle=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752d544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.3320, -1.1011,  ..., -1.1666, -0.5075, -0.1907],\n",
       "        [ 0.0000, -0.3320, -1.1011,  ...,  0.3638, -0.5075, -0.1907],\n",
       "        [ 0.0000, -0.3320, -0.6751,  ..., -1.1666, -0.5075, -0.1907],\n",
       "        ...,\n",
       "        [ 0.0000, -0.3320, -1.1011,  ...,  1.0440,  2.4412,  1.0069],\n",
       "        [ 0.0000, -0.3320,  0.3900,  ..., -0.3164, -0.5075, -0.1907],\n",
       "        [ 0.0000, -0.3320, -0.2491,  ...,  0.1938, -0.5075, -0.1907]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after AL selection and finetuning: 73.56%\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "model = models[0]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)\n",
    "\n",
    "def fast_jacobian(model, x):\n",
    "    model.zero_grad()\n",
    "    # ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)  # (1, input_dim)\n",
    "    b = x.shape[0]\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # precompute param sizes to flatten consistently\n",
    "    param_numels = [p.numel() for p in params]\n",
    "\n",
    "    J_batch = []\n",
    "    for i in range(b):\n",
    "        xi = x[i : i + 1]  # keep batch dim for forward\n",
    "        yi = model(xi)  # (1, d_out)\n",
    "        d_out = yi.shape[-1]\n",
    "\n",
    "        # compute grads for this sample: result (d_out, n_params)\n",
    "        J_i_rows = []\n",
    "        for k in range(d_out):\n",
    "            grads = torch.autograd.grad(yi[0, k], params, retain_graph=True)\n",
    "            grad_flat = torch.cat([g.reshape(-1) for g in grads])\n",
    "            J_i_rows.append(grad_flat)\n",
    "        J_i = torch.stack(J_i_rows, dim=0)  # (d_out, n_params)\n",
    "        J_batch.append(J_i)\n",
    "\n",
    "    J_batch = torch.stack(J_batch, dim=0)  # (b, d_out, n_params)\n",
    "    return J_batch\n",
    "\n",
    "def compute_outcome_hessian_from_model(model, inputs):\n",
    "    # inputs: (d,) or (b, d)\n",
    "    if inputs.ndim == 1:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    z = model(inputs)  # (b, d_out)\n",
    "    p = torch.softmax(z, dim=-1)  # (b, d_out)\n",
    "    # diag_embed builds (b, d_out, d_out)\n",
    "    H = torch.diag_embed(p) - p.unsqueeze(2) * p.unsqueeze(1)  # (b, d_out, d_out)\n",
    "    return H\n",
    "\n",
    "def symmetric_matrix_sqrt(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A: (n,n) or (batch, n, n)\n",
    "    returns: A_sqrt with same shape\n",
    "    \"\"\"\n",
    "    single = (A.dim() == 2)\n",
    "    if single:\n",
    "        A = A.unsqueeze(0)\n",
    "    w, v = torch.linalg.eigh(A)\n",
    "    w_clamped = torch.clamp(w, min=eps)\n",
    "    w_sqrt = torch.sqrt(w_clamped)\n",
    "    A_sqrt = (v * w_sqrt.unsqueeze(-2)) @ v.transpose(-2, -1)\n",
    "    if single:\n",
    "        return A_sqrt[0]\n",
    "    return A_sqrt\n",
    "\n",
    "def low_rank_updated_part(model, x, return_batch: bool = False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - if return_batch=True: U_batch of shape (b, n_params, d_out)\n",
    "      - else: U_all of shape (n_params, b * d_out)  (backward-compatible)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    H = compute_outcome_hessian_from_model(model, x)    # (b, d_out, d_out)\n",
    "    J_batch = fast_jacobian(model, x)                   # (b, d_out, n_params)\n",
    "    H_sqrt = symmetric_matrix_sqrt(H)                   # (b, d_out, d_out)\n",
    "\n",
    "    # J_batch: (b, d_out, n_params) -> transpose -> (b, n_params, d_out)\n",
    "    Jt = J_batch.transpose(1, 2)\n",
    "    U_batch = torch.matmul(Jt, H_sqrt)                  # (b, n_params, d_out)\n",
    "\n",
    "    if return_batch:\n",
    "        return U_batch\n",
    "    b, n_params, d_out = U_batch.shape\n",
    "    U_all = U_batch.permute(1, 0, 2).reshape(n_params, b * d_out)\n",
    "    return U_all\n",
    "\n",
    "def DoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute D-opt score per input sample.\n",
    "    - x: (input_dim,) or (b, input_dim)\n",
    "    - Hessian: torch tensor (n_params,) (diagonal)\n",
    "    Returns: torch tensor shape (b,) with per-sample log-determinant scores\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        # use slogdet for stability\n",
    "        sign, ld = torch.linalg.slogdet(A)\n",
    "        # if numeric issue (sign <=0) return nan for that sample\n",
    "        scores.append(ld if sign > 0 else torch.tensor(float('nan'), device=A.device))\n",
    "    return torch.stack(scores)  # (b,)\n",
    "\n",
    "def AoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute A-opt reduction per input sample.\n",
    "    Returns: torch tensor shape (b,) with per-sample Delta values.\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    deltas = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        A = A + eps * torch.eye(A.shape[0], device=A.device)\n",
    "        S = C.T @ C                          # (d_out, d_out)\n",
    "        X = torch.linalg.solve(A, S)         # (d_out, d_out)\n",
    "        deltas.append(torch.trace(X))\n",
    "    return torch.stack(deltas)  # (b,)\n",
    "\n",
    "def train_model(model, train_set_Labeled, criterion, optimizer, n_epochs=10):\n",
    "    n_epochs_monitor = n_epochs\n",
    "    for epoch in range(n_epochs_monitor):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_set_Labeled:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    return model\n",
    "\n",
    "def return_hessian_eigenvalues(model, train_set_Labeled):\n",
    "    la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "    train_loader = DataLoader(train_set_Labeled, batch_size=32, shuffle=True)\n",
    "    la.fit(train_loader)\n",
    "    val = la.H\n",
    "    return val\n",
    "\n",
    "def selection_AL_scores(model, unlabeled_set, Hessian, score_type='Aopt', top_k=10):\n",
    "    \"\"\"\n",
    "    Select top_k samples from unlabeled_set based on AL scores.\n",
    "    score_type: 'Aopt' or 'Dopt'\n",
    "    Returns: indices of selected samples in unlabeled_set\n",
    "    \"\"\"\n",
    "    if score_type == 'Aopt':\n",
    "        scores = AoptScore_per_sample(model, unlabeled_set[:][0], Hessian)\n",
    "    elif score_type == 'Dopt':\n",
    "        scores = DoptScore_per_sample(model, unlabeled_set[:][0], Hessian)\n",
    "    else:\n",
    "        raise ValueError(\"score_type must be 'Aopt' or 'Dopt'\")\n",
    "\n",
    "    # Get top_k indices\n",
    "    topk_indices = torch.topk(scores, k=top_k).indices\n",
    "    return topk_indices.tolist()\n",
    "\n",
    "def delete_samples_from_unlabeled(unlabeled_set, selected_indices):\n",
    "    \"\"\"\n",
    "    Remove samples at selected_indices from unlabeled_set.\n",
    "    Returns: new Subset of unlabeled_set without selected samples.\n",
    "    \"\"\"\n",
    "    all_indices = list(range(len(unlabeled_set)))\n",
    "    remaining_indices = [i for j, i in enumerate(all_indices) if j not in selected_indices]\n",
    "    new_unlabeled_set = Subset(unlabeled_set.dataset, [unlabeled_set.indices[i] for i in remaining_indices])\n",
    "    return new_unlabeled_set\n",
    "\n",
    "def add_samples_to_labeled(labeled_set, unlabeled_set, selected_indices):\n",
    "    \"\"\"\n",
    "    Add samples at selected_indices from unlabeled_set to labeled_set.\n",
    "    Returns: new Subset of labeled_set with added samples.\n",
    "    \"\"\"\n",
    "    new_indices = labeled_set.indices + [unlabeled_set.indices[i] for i in selected_indices]\n",
    "    new_labeled_set = Subset(labeled_set.dataset, new_indices)\n",
    "    return new_labeled_set\n",
    "\n",
    "def add_and_delete_samples(labeled_set, unlabeled_set, selected_indices):\n",
    "    new_labeled_set = add_samples_to_labeled(labeled_set, unlabeled_set, selected_indices)\n",
    "    new_unlabeled_set = delete_samples_from_unlabeled(unlabeled_set, selected_indices)\n",
    "    return new_labeled_set, new_unlabeled_set\n",
    "\n",
    "def AL_finetune_model(model, AL_train_set_Labeled, criterion, optimizer, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in AL_train_set_Labeled:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "model = train_model(model, train_set_Labeled, criterion, optimizer, n_epochs=10)\n",
    "Hessian = return_hessian_eigenvalues(model, train_set_Labeled)\n",
    "selected_indices = selection_AL_scores(model, train_set_Unlabeled, Hessian, score_type='Aopt', top_k=10)\n",
    "train_set_Labeled, train_set_Unlabeled = add_and_delete_samples(train_set_Labeled, train_set_Unlabeled, selected_indices)\n",
    "model = AL_finetune_model(model, train_set_Labeled, criterion, optimizer, n_epochs=10)\n",
    "test_accuracy = evaluate(test_loader, model)\n",
    "print(f\"Test Accuracy after AL selection and finetuning: {test_accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x160842f5f30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16a25353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(train_set) - 256) :\\n    x = train_set[256 + i][0]\\n    Values.append([DoptScore(model, x, Hessian).item(), AoptScore(model, x, Hessian).item()])\\nValues = np.array(Values)\\nprint(Values.shape)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fast_jacobian(model, x):\n",
    "    model.zero_grad()\n",
    "    # ensure batch dimension\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)  # (1, input_dim)\n",
    "    b = x.shape[0]\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # precompute param sizes to flatten consistently\n",
    "    param_numels = [p.numel() for p in params]\n",
    "\n",
    "    J_batch = []\n",
    "    for i in range(b):\n",
    "        xi = x[i : i + 1]  # keep batch dim for forward\n",
    "        yi = model(xi)  # (1, d_out)\n",
    "        d_out = yi.shape[-1]\n",
    "\n",
    "        # compute grads for this sample: result (d_out, n_params)\n",
    "        J_i_rows = []\n",
    "        for k in range(d_out):\n",
    "            grads = torch.autograd.grad(yi[0, k], params, retain_graph=True)\n",
    "            grad_flat = torch.cat([g.reshape(-1) for g in grads])\n",
    "            J_i_rows.append(grad_flat)\n",
    "        J_i = torch.stack(J_i_rows, dim=0)  # (d_out, n_params)\n",
    "        J_batch.append(J_i)\n",
    "\n",
    "    J_batch = torch.stack(J_batch, dim=0)  # (b, d_out, n_params)\n",
    "    return J_batch\n",
    "\n",
    "def compute_outcome_hessian_from_model(model, inputs):\n",
    "    # inputs: (d,) or (b, d)\n",
    "    if inputs.ndim == 1:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    z = model(inputs)  # (b, d_out)\n",
    "    p = torch.softmax(z, dim=-1)  # (b, d_out)\n",
    "    # diag_embed builds (b, d_out, d_out)\n",
    "    H = torch.diag_embed(p) - p.unsqueeze(2) * p.unsqueeze(1)  # (b, d_out, d_out)\n",
    "    return H\n",
    "\n",
    "def symmetric_matrix_sqrt(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    A: (n,n) or (batch, n, n)\n",
    "    returns: A_sqrt with same shape\n",
    "    \"\"\"\n",
    "    single = (A.dim() == 2)\n",
    "    if single:\n",
    "        A = A.unsqueeze(0)\n",
    "    w, v = torch.linalg.eigh(A)\n",
    "    w_clamped = torch.clamp(w, min=eps)\n",
    "    w_sqrt = torch.sqrt(w_clamped)\n",
    "    A_sqrt = (v * w_sqrt.unsqueeze(-2)) @ v.transpose(-2, -1)\n",
    "    if single:\n",
    "        return A_sqrt[0]\n",
    "    return A_sqrt\n",
    "\n",
    "def low_rank_updated_part(model, x, return_batch: bool = False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - if return_batch=True: U_batch of shape (b, n_params, d_out)\n",
    "      - else: U_all of shape (n_params, b * d_out)  (backward-compatible)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    H = compute_outcome_hessian_from_model(model, x)    # (b, d_out, d_out)\n",
    "    J_batch = fast_jacobian(model, x)                   # (b, d_out, n_params)\n",
    "    H_sqrt = symmetric_matrix_sqrt(H)                   # (b, d_out, d_out)\n",
    "\n",
    "    # J_batch: (b, d_out, n_params) -> transpose -> (b, n_params, d_out)\n",
    "    Jt = J_batch.transpose(1, 2)\n",
    "    U_batch = torch.matmul(Jt, H_sqrt)                  # (b, n_params, d_out)\n",
    "\n",
    "    if return_batch:\n",
    "        return U_batch\n",
    "    b, n_params, d_out = U_batch.shape\n",
    "    U_all = U_batch.permute(1, 0, 2).reshape(n_params, b * d_out)\n",
    "    return U_all\n",
    "\n",
    "def DoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute D-opt score per input sample.\n",
    "    - x: (input_dim,) or (b, input_dim)\n",
    "    - Hessian: torch tensor (n_params,) (diagonal)\n",
    "    Returns: torch tensor shape (b,) with per-sample log-determinant scores\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    scores = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        # use slogdet for stability\n",
    "        sign, ld = torch.linalg.slogdet(A)\n",
    "        # if numeric issue (sign <=0) return nan for that sample\n",
    "        scores.append(ld if sign > 0 else torch.tensor(float('nan'), device=A.device))\n",
    "    return torch.stack(scores)  # (b,)\n",
    "\n",
    "def AoptScore_per_sample(model, x, Hessian, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute A-opt reduction per input sample.\n",
    "    Returns: torch tensor shape (b,) with per-sample Delta values.\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    U_batch = low_rank_updated_part(model, x, return_batch=True)   # (b, n_params, d_out)\n",
    "    Hinv = 1.0 / (Hessian + eps)                                   # (n_params,)\n",
    "\n",
    "    deltas = []\n",
    "    for i in range(U_batch.shape[0]):\n",
    "        U_i = U_batch[i]                    # (n_params, d_out)\n",
    "        C = Hinv.unsqueeze(1) * U_i         # (n_params, d_out)\n",
    "        A = torch.eye(U_i.shape[1], device=U_i.device) + (U_i.T @ C)  # (d_out, d_out)\n",
    "        A = A + eps * torch.eye(A.shape[0], device=A.device)\n",
    "        S = C.T @ C                          # (d_out, d_out)\n",
    "        X = torch.linalg.solve(A, S)         # (d_out, d_out)\n",
    "        deltas.append(torch.trace(X))\n",
    "    return torch.stack(deltas)  # (b,)\n",
    "'''\n",
    "\n",
    "models = get_models()\n",
    "model = models[0]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "indices = list(range(len(train_set)))\n",
    "random.shuffle(indices)\n",
    "subset_indices = indices[:256]\n",
    "train_subset = Subset(train_set, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "n_epochs_monitor = 30\n",
    "for epoch in range(n_epochs_monitor):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "la = Laplace(model, 'classification', subset_of_weights='all', hessian_structure='diag')\n",
    "la.fit(train_loader)\n",
    "Hessian = la.H\n",
    "Values = []\n",
    "x = train_set[0]\n",
    "'''\n",
    "'''\n",
    "for i in range(len(train_set) - 256) :\n",
    "    x = train_set[256 + i][0]\n",
    "    Values.append([DoptScore(model, x, Hessian).item(), AoptScore(model, x, Hessian).item()])\n",
    "Values = np.array(Values)\n",
    "print(Values.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c18d4735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22.7606], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_set[0][0]\n",
    "AoptScore_per_sample(model, x, Hessian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doyoung_laplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
